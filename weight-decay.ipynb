{"cells":[{"cell_type":"markdown","metadata":{},"source":["This is a copy of my Pytorch-titanic Notebook but it will apply weight decay to our linear model"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-12-15T18:36:11.577650Z","iopub.status.busy":"2023-12-15T18:36:11.576570Z","iopub.status.idle":"2023-12-15T18:36:11.587718Z","shell.execute_reply":"2023-12-15T18:36:11.586441Z","shell.execute_reply.started":"2023-12-15T18:36:11.577565Z"},"trusted":true},"outputs":[],"source":["import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","import os\n","\n","is_kaggle = \"KAGGLE_WORKING_DIR\" in os.environ or \"/kaggle\" in os.getcwd()\n","print(\"Running on Kaggle:\", is_kaggle)\n","\n","if is_kaggle:\n","    data_path = \"/kaggle/input/titanic/\"\n","else:\n","    data_path = os.getcwd() + \"/\""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch\n","np.set_printoptions(linewidth=140)\n","torch.set_printoptions(linewidth=140, sci_mode=False, edgeitems=7)\n","pd.set_option('display.width', 140)"]},{"cell_type":"markdown","metadata":{},"source":["Based on fast.ai chapter 5 we'll now iterate on the numpy-titanic notebook by using pytorch and applying some best practices from that chapter"]},{"cell_type":"markdown","metadata":{},"source":["## Prepare Data set"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-15T18:36:11.590356Z","iopub.status.busy":"2023-12-15T18:36:11.590031Z","iopub.status.idle":"2023-12-15T18:36:11.627922Z","shell.execute_reply":"2023-12-15T18:36:11.626658Z","shell.execute_reply.started":"2023-12-15T18:36:11.590329Z"},"trusted":true},"outputs":[],"source":["df = pd.read_csv(data_path + \"train.csv\")\n","df"]},{"cell_type":"markdown","metadata":{},"source":["### Handling na values\n","For linear regression to work we need numerical values, n/a values are not numerical so we should check if our data set contain them."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df.isna().sum()"]},{"cell_type":"markdown","metadata":{},"source":["We should avoid removing columns or rows. Even the absence of data can sometimes indicate a pattern.\n","\n","There are many ways to substitute na_values, the easiest of which is to replace na values with the mode value (the most commonly occuring value). This is a good starting point as usually the method of substituion doesn't have a large impact on our results so the mode is good to get an MVP up and running we can iterate on."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["modes = df.mode().iloc[0]\n","modes"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df.fillna(modes, inplace=True)\n","df.isna().sum()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def substitue_na_with_modes(df: pd.DataFrame) -> pd.DataFrame:\n","    modes = df.mode().iloc[0]\n","    return df.fillna(modes)"]},{"cell_type":"markdown","metadata":{},"source":["### Converting Category Data to Binary Categorical Values\n"]},{"cell_type":"markdown","metadata":{},"source":["We can get view our non-numeric or numberic data using the describe function.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df.describe(include=[object])"]},{"cell_type":"markdown","metadata":{},"source":["Sex and Embarked only have 2, and 3 unique values respectively. It's safe to say these are categorical values.\n","\n","We should also check if any of our numbers are categorical"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df.describe(include=[np.number])"]},{"cell_type":"markdown","metadata":{},"source":["We can see from its quarile values that PClass is likely also categorical despite being numeric as its only values are 1, 2 or 3. We can confirm this by looking at the [data dictionary](https://www.kaggle.com/competitions/titanic/data) for the kaggle competition and by via pandas.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df.Pclass.unique()"]},{"cell_type":"markdown","metadata":{},"source":["\n","Sex, the Passenger class and Embarking city are not measurable attributes so we should convert them to Boolean numbers that can be used as co-efficients. In the previous notebook we did this manually however this pandas can do this for us using `Dataframe.get_dummies()`"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-15T18:36:11.664903Z","iopub.status.busy":"2023-12-15T18:36:11.664551Z","iopub.status.idle":"2023-12-15T18:36:11.691977Z","shell.execute_reply":"2023-12-15T18:36:11.690666Z","shell.execute_reply.started":"2023-12-15T18:36:11.664869Z"},"trusted":true},"outputs":[],"source":["categorical_feature_names = ['Sex', 'Embarked', 'Pclass']\n","df = pd.get_dummies(df, columns=categorical_feature_names, dtype=int)\n","df.columns"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dummy_column_names = ['Sex_male', 'Sex_female', 'Pclass_1', 'Pclass_2', 'Pclass_3', 'Embarked_C', 'Embarked_Q',\n","       'Embarked_S']\n","df[dummy_column_names].head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def convert_categories_to_binary_values(df: pd.DataFrame) -> pd.DataFrame:\n","    categorical_feature_names = ['Sex', 'Embarked', 'Pclass']\n","    return pd.get_dummies(df, columns=categorical_feature_names, dtype=int)"]},{"cell_type":"markdown","metadata":{},"source":["### Handling long-tail numerical data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import matplotlib\n","df.Fare.hist()"]},{"cell_type":"markdown","metadata":{},"source":["The `Fare` column has lots of small values with the occasional very large value. Uniform normalization using the max value isn't ideal when we're dealing with lots of small values with occasional very large values as the variation between the lower numbers will be lost. To normalize the values we can use a log function (log10 here) to bring the numbers down to reasonable ranges. We must use `log10(x+1)` to avoid 0 values as `log10(0)` would give us infinity."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-15T18:36:11.771533Z","iopub.status.busy":"2023-12-15T18:36:11.771187Z","iopub.status.idle":"2023-12-15T18:36:11.795392Z","shell.execute_reply":"2023-12-15T18:36:11.794544Z","shell.execute_reply.started":"2023-12-15T18:36:11.771504Z"},"trusted":true},"outputs":[],"source":["import math\n","df['LogFare'] = np.log(df['Fare'] + 1)\n","df.LogFare.hist()"]},{"cell_type":"markdown","metadata":{},"source":["## Linear Regression"]},{"cell_type":"markdown","metadata":{},"source":["### Pytorch Tensors\n","For our gradient descent we'll be using Pytorch rather than numpy for this workbook as it will do a lot of the heavy lifting for us. Alongside Tensorflow pytorch is the most commonly used framework for machine learning."]},{"cell_type":"markdown","metadata":{},"source":["We'll start by creating Tensors for our target values (known survival status) and features (our numerical data)."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from torch import tensor\n","target_tensor = tensor(df.Survived)\n","target_tensor"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["feature_names = ['Age', 'SibSp', 'Parch', 'LogFare'] + dummy_column_names\n","feature_df = df[feature_names]\n","feature_df"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["features = feature_df.values\n","feature_tensor = tensor(features, dtype=torch.float)\n","feature_tensor"]},{"cell_type":"markdown","metadata":{},"source":["### Normalization\n","Once all our features are numerical we need to ensure they're somewhat uniform. For Linear regression and many other ML methods having some features be much larger than others will disrupt the process. Rather than do this manually we can have Pytorch do this for us."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["max_values, max_indices = feature_tensor.max(dim=0)\n","max_values"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["feature_tensor = feature_tensor / max_values\n","feature_tensor"]},{"cell_type":"markdown","metadata":{},"source":["#### Broadcasting\n","`feature_tensor / max_values` is an example of [broadcasting](https://pytorch.org/docs/stable/notes/broadcasting.html). \n","`max_values` is a one dimensional vector with shape (12). `feature_tensor` is a 2 dimensional matrix with shape (892,12). Because `max_values` is the same size as one of `feature_tensor`'s it will be applied to all 891 rows of `feature_tensor`\n","\n","Broadcasting is useful for large datasets. The calculations are optimized and run on a GPU when available."]},{"cell_type":"markdown","metadata":{},"source":["### Prepare initial linear co-efficient values\n","For linear regression we'd like a one dimensional vector of coefficients equal to our number of rows. Unlike in previous examples we don't need a constant as our dummy variables effectively act as a constant."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-15T18:36:11.820322Z","iopub.status.busy":"2023-12-15T18:36:11.819991Z","iopub.status.idle":"2023-12-15T18:36:11.833908Z","shell.execute_reply":"2023-12-15T18:36:11.832743Z","shell.execute_reply.started":"2023-12-15T18:36:11.820295Z"},"trusted":true},"outputs":[],"source":["torch.manual_seed(442)\n","feature_count = feature_tensor.shape[1]\n","coefficients = torch.rand(feature_count) - 0.5\n","coefficients"]},{"cell_type":"markdown","metadata":{},"source":["Generally we don't want to set a manual seed so we can be aware of how stable our data is or isn't. However for the sake of this lesson I'd like to check I'm getting consistent results with the lesson plan."]},{"cell_type":"markdown","metadata":{},"source":["### Create Predictions\n","We calculate the linear function of our parameters by multiplied them against our random Coefficients then summing each row of weighted values up to create a prediction for each passenger\n","Pytorch's broadcasting can once again be used here to simplify things considerably. We'll print it out to check there aren't any weighted values that are significantly oversized."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-15T18:36:11.836343Z","iopub.status.busy":"2023-12-15T18:36:11.835997Z","iopub.status.idle":"2023-12-15T18:36:11.858845Z","shell.execute_reply":"2023-12-15T18:36:11.857637Z","shell.execute_reply.started":"2023-12-15T18:36:11.836315Z"},"trusted":true},"outputs":[],"source":["weighted_values = feature_tensor * coefficients\n","weighted_values[:4]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["predictions = weighted_values.sum(dim=1)\n","predictions[:10]"]},{"cell_type":"markdown","metadata":{},"source":["### Calculate loss\n","Our loss here is the average difference between our prediction value and whether the passegner survived or not (1 or 0)."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["loss = torch.abs(predictions - target_tensor).mean()\n","loss"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def create_predictions(features: torch.Tensor, coefficients: torch.Tensor) -> torch.Tensor:\n","    return (coefficients * features).sum(dim=1)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def calculate_loss(features: torch.Tensor, coefficients: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n","    predictions = create_predictions(features, coefficients=coefficients)\n","    return torch.abs(predictions - targets).mean()"]},{"cell_type":"markdown","metadata":{},"source":["### Doing a single Gradient Descent step\n","Now we want to optimize our loss with gradient descent. This too will be significantly easier using Pytorch as it will calculate the gradient for us.\n","\n","We must tell pytorch to store the results of each coefficient calculation so we can get the gradients from it later."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-15T18:36:11.860841Z","iopub.status.busy":"2023-12-15T18:36:11.860409Z","iopub.status.idle":"2023-12-15T18:36:22.655866Z","shell.execute_reply":"2023-12-15T18:36:22.654577Z","shell.execute_reply.started":"2023-12-15T18:36:11.860799Z"},"trusted":true},"outputs":[],"source":["coefficients.requires_grad_()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["loss = calculate_loss(feature_tensor, coefficients=coefficients, targets=target_tensor)\n","loss"]},{"cell_type":"markdown","metadata":{},"source":["The loss is in a tensor where can ask Pytorch to calculate the gradient by calling `backward()`"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["loss.backward()\n","coefficients.grad"]},{"cell_type":"markdown","metadata":{},"source":["Here we perform one gradient descent step\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["loss = calculate_loss(feature_tensor, coefficients=coefficients, targets=target_tensor)\n","loss.backward\n","with torch.no_grad():\n","    assert coefficients.grad is not None\n","    coefficients.sub_(coefficients.grad * 0.1)\n","    coefficients.grad.zero_()\n","    print(calculate_loss(feature_tensor, coefficients=coefficients, targets=target_tensor))"]},{"cell_type":"markdown","metadata":{},"source":["A few points:\n","1. `torch.no_grad()` is required to ensure the parameter update step is peformed without tracking gradients. We want to track gradients for the forward and backward steps but not when directly modifying the parameters\n","2. `coefficients.sub_(coefficients.grad * 0.1)` reduces the coefficients by their gradient to the loss. More significant features will be reduced more. \n","3. Both `sub_` and `zero_` operations are done in place for memory efficiency and to preserve the tensors memory graph (this is also ensured by `torch.no_grad()` although it's good practice when working with tensors).\n","4. `coefficients.grad.zero_()` sets our gradients to zero. This is necessary as if we were to do another backpass the new gradients would be added to the old ones."]},{"cell_type":"markdown","metadata":{},"source":["### Creating a validation set\n"]},{"cell_type":"markdown","metadata":{},"source":["Before we begin training we need a validation set to compare our training data against."]},{"cell_type":"markdown","metadata":{},"source":["I've deviated from the [fast.ai kaggle workbook](https://www.kaggle.com/code/jhoward/linear-model-and-neural-net-from-scratch) as they split their validation set using the fastai library to keep things consistent for their next chapter. I'm interested in primarily learning Pytorch so I'm going to split the dataset without the fastai library. However so I can check if my results match fast.ai's I'm going to include their splitter here it will be used if `use_fastai_splitter` is set to `True` so I can check my results are consistent with the fast.ai tutorials."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from random import Random\n","from numpy import int64\n","from fastai.data.transforms import RandomSplitter\n","from typing import Tuple, List, cast\n","from fastcore.foundation import L\n","from torch import Tensor\n","\n","def split_data_with_fastai(df: pd.DataFrame) -> Tuple[Tensor,Tensor]:\n","    train_indices, validation_indices = RandomSplitter(seed=42)(df)\n","    return torch.tensor(train_indices, dtype=torch.int64), torch.tensor(validation_indices, dtype=torch.int64)"]},{"cell_type":"markdown","metadata":{},"source":["First we'll split our data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["use_fastai_splitter = True\n","total_passengers = feature_tensor.size(0)\n","training_set_size = int(total_passengers * 0.8)\n","\n","if use_fastai_splitter:\n","    train_indices, validation_indices = split_data_with_fastai(df)\n","else:\n","    randomized_indices = torch.randperm(total_passengers)\n","    train_indices = randomized_indices[:training_set_size]\n","    validation_indices = randomized_indices[training_set_size:]\n","\n","training_features = feature_tensor[train_indices]\n","validation_features = feature_tensor[validation_indices]\n","training_targets = target_tensor[train_indices]\n","validation_targets = target_tensor[validation_indices]\n","len(training_features), len(validation_features)"]},{"cell_type":"markdown","metadata":{},"source":["This note book doesn't use Pytorch's `Dataset`s. We'd likely use these in a real project although for this example we're keeping things a bit barer than normal so we can see the process."]},{"cell_type":"markdown","metadata":{},"source":["We'll add what we've done so far in to functions to make things easier to read and re-usable."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def update_coefficients(coefficients, learning_rate):\n","    coefficients.sub_(coefficients.grad * learning_rate)\n","    coefficients.grad.zero_()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def one_epoch(coefficients, learning_rate):\n","    loss = calculate_loss(training_features, coefficients, training_targets)\n","    loss.backward()\n","    with torch.no_grad():\n","        update_coefficients(coefficients, learning_rate=learning_rate)\n","        \n","    print(f\"{loss:.3f}\", end=\"; \")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def generate_coefficients(features: torch.Tensor) -> torch.Tensor:\n","    coefficient_count = features.shape[1]\n","    coefficients = torch.rand(coefficient_count) - 0.5\n","    coefficients.requires_grad_()\n","    return coefficients"]},{"cell_type":"markdown","metadata":{},"source":["Now to train the model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def train_model(epoch_count=30, learning_rate=0.1):\n","    coefficients = generate_coefficients(training_features)\n","    for i in range(epoch_count):\n","        one_epoch(coefficients, learning_rate=learning_rate)\n","    return coefficients"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["coefficients = train_model(epoch_count=18, learning_rate=0.2)\n","coefficients"]},{"cell_type":"markdown","metadata":{},"source":["We can see below that our models has optimized our weights to reduce our loss. From this we can see that the model believes"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def show_coeffs(): \n","    coeff_array = [coeff.item() for coeff in coefficients]\n","    coeff_df = pd.DataFrame({'Feature': feature_names, 'Coefficient': coeff_array})\n","    display(coeff_df)\n","show_coeffs()"]},{"cell_type":"markdown","metadata":{},"source":["### Measuring accuracy"]},{"cell_type":"markdown","metadata":{},"source":["To view our accuracy we'll now use our validation set. We'll create predictions using our newly trained coefficients and see how accurate they are."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["predictions = create_predictions(validation_features, coefficients=coefficients)\n","predictions[:10]"]},{"cell_type":"markdown","metadata":{},"source":["If our predictions is >0.5 and the passegner surivied we're correct. If the passenger died we want a prediction < 0.5. 0 = died, 1 = survived. This code merely rounds our predictions to whichever of these values is closest"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["results = validation_targets.bool() == (predictions>0.5)\n","results.float().mean()"]},{"cell_type":"markdown","metadata":{},"source":["We're 79% accurate which is pretty good going."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from torch import Tensor\n","\n","\n","def calculate_accuracy(coefficients, features: torch.Tensor) -> Tensor:\n","    predictions = create_predictions(features, coefficients=coefficients)\n","    results = validation_targets.bool() == (predictions>0.5)\n","    return results.float().mean()"]},{"cell_type":"markdown","metadata":{},"source":["### Sigmoid\n","When creating predictions that are between 0 and 1 we can increase our accuracy by using the sigmoid function which moves all our values between 0 and 1 and larger negative or positives values will respectively asymptotically converge towards 0 or 1."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import sympy\n","sympy.plot(\"1/(1+exp(-x))\", xlim=(-5,5))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def create_predictions(features: torch.Tensor, coefficients: torch.Tensor) -> torch.Tensor:\n","    summed_weighted_values = (coefficients * features).sum(dim=1)\n","    return torch.sigmoid(summed_weighted_values)\n"]},{"cell_type":"markdown","metadata":{},"source":["Constricting the range of our predictions within the range of what they can realistically be makes them much easier to optimize. When this is applied to every prediction each epoch should minimize our loss more effectivelyby by eliminating values that are outside the range of what our predictions can realistically be.\n","\n","This in turn allows us to substantially increase the learning rate as our loss won't be as high or fluctuate as wildly."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["coefficients = train_model(learning_rate=100)\n","calculate_accuracy(coefficients, features=validation_features)"]},{"cell_type":"markdown","metadata":{},"source":["83% a sharp improvement!"]},{"cell_type":"markdown","metadata":{},"source":["## Weight Decay\n","Often our loss will go down but our validation loss will begin to increase. This is usually a sign of overfitting. One of those most basic ways to prevent overfitting is weight decay.\n","\n","We add all the weights squared to our loss. This will hinder our training but helps prevent overfitting by forcing our weights to get smaller. Smaller weights mean less resolution in our models solutions, as demonstrated a solution that fits our training data too closely will over-fit\n","\n","![overfitting-illustration](overfitting-example.webp)"]},{"cell_type":"markdown","metadata":{},"source":["It will probably be quite difficult to make this model overfit as its quite a simply model and doesn't have many parameters so for now we'll just print out our validation loss alongside our loss so we can at least compare the two side by side. Having your loss improve but your validation loss worsen is usually a sign of overfitting."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def one_epoch(coefficients, learning_rate):\n","    loss = calculate_loss(training_features, coefficients, training_targets)\n","    loss.backward()\n","    with torch.no_grad():\n","        update_coefficients(coefficients, learning_rate=learning_rate)\n","        validaton_loss = calculate_loss(validation_features, coefficients, validation_targets)\n","        \n","    print(f\"loss: {loss:.3f}, val_loss: {validaton_loss}\", end=\";\\n\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["coefficients = train_model(learning_rate=100)\n","calculate_accuracy(coefficients, features=validation_features)"]},{"cell_type":"markdown","metadata":{},"source":["## Prepare Submission CSV"]},{"cell_type":"markdown","metadata":{},"source":["### Using a Test set\n","Before submitting to Kaggle we'll want to test the effectiveness of our data against our test set. \n","#### Why not use the Validation set?\n","This may seem similar to how we used our validation set but there's an important difference. A validation set is used to give us an unbiased evaluation of our model's performance. Unlike a training set which is biased as we're training our model on it. As we develop our model the validation set will indirectly become biased as we iterate on our model to improve the validation sets accuracy. The test set is only ever used once we have finished developing our model so it gives us an accurate assessment of how our model behaves on completely unseen data.\n","\n","- Training set - Model bias\n","- Validation Set - Developer bias\n","- Test set - No bias"]},{"cell_type":"markdown","metadata":{},"source":["### Clean Test Data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test_df = pd.read_csv(data_path + 'test.csv')\n","test_df.isna().sum()"]},{"cell_type":"markdown","metadata":{},"source":["We can use the same steps we took for cleaning the training data on our test data. However it's always worth checking if there are any additional na values. Here there's an na value for Fare that needs resolving."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test_df['Fare'] = test_df.Fare.fillna(0)\n","test_df = substitue_na_with_modes(test_df)\n","test_df = convert_categories_to_binary_values(test_df)\n","test_df[\"LogFare\"] = np.log(test_df['Fare'] + 1)\n","test_df.isna().sum()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test_df[feature_names][:5]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test_features = tensor(test_df[feature_names].values, dtype=torch.float)\n","test_features = test_features / max_values\n","test_features[:3]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["feature_tensor"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["coefficients"]},{"cell_type":"markdown","metadata":{},"source":["### Create test predictions"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["with torch.no_grad():\n","    test_predictions = create_predictions(test_features, coefficients=coefficients)\n","test_predictions = (test_predictions > 0.5).int()\n","test_predictions"]},{"cell_type":"markdown","metadata":{},"source":["### Create Kaggle submission\n","We can view the sample submission kaggle has given us to see how to format this."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sample_df = pd.read_csv(data_path + \"gender_submission.csv\")\n","sample_df[:3]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["submission_df = pd.DataFrame({ \"PassengerId\": test_df[\"PassengerId\"], \"Survived\": test_predictions })\n","submission = submission_df.to_csv(\"submission.csv\",index=False)\n","!head submission.csv"]},{"cell_type":"markdown","metadata":{},"source":["## Neural Nets\n","The calculation above was a linear regression as we only use one set of parameters in the form of our features which we immedietely derived the output from. This effectively gave us a single layer with one neuron.\n","\n","Here we'll employ two additional concepts to improve the resolution of our calculations accuracy. We will create a single hidden layer with multiple neurons, apply a RELU (Rectified Linear Unit) function and add them together to give us a loss. A RELU function is non-linear and simply replaces every negative number with a 0."]},{"cell_type":"markdown","metadata":{},"source":["### Create Matrix of Coefficients"]},{"cell_type":"markdown","metadata":{},"source":["Prior to now we were using a single neuron consisting of a single layer. We're now going to add additional neurons by generating more than one set of coefficients\n","\n","Our first layer will take our inputs/features and create 20 outputs from 20 different sets of coefficients."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["hidden_layer_neuron_count = 20\n","layer1 = torch.rand(feature_count, hidden_layer_neuron_count) - 0.5\n","layer1.shape"]},{"cell_type":"markdown","metadata":{},"source":["When we've more than one neuron we need to adjust the values of each neuron proportionally so we end up with a similar magnitude."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["layer1 = layer1 / hidden_layer_neuron_count\n","layer1[:3]"]},{"cell_type":"markdown","metadata":{},"source":["### Creating an output layer\n","The 2nd layer is our output layer which will produce one output predicting the passengers survival. As we're using neurons from a hidden layer as our input and not our features which included dummy values we now need to include a constant (also known as a bias). This 2nd layer consists of one neuron"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["layer2 = torch.rand(hidden_layer_neuron_count, 1) - 0.3\n","constant = torch.rand(1)[0]\n","layer2.shape"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def generate_coefficients(features: torch.Tensor, hidden_layer_neuron_count: int=20):\n","    feature_count = features.shape[1]\n","    layer1 = (torch.rand(feature_count, hidden_layer_neuron_count) - 0.5) / feature_count\n","    layer2 = torch.rand(hidden_layer_neuron_count, 1) - 0.3\n","    constant = torch.rand(1)[0]\n","    return layer1.requires_grad_(), layer2.requires_grad_(), constant.requires_grad_()"]},{"cell_type":"markdown","metadata":{},"source":["### Convert targets to column vectors\n","Lastly we need to add an additional dimension to our targets so matrix multiplications will work correctly on them.  We need to be very careful to remember to do this as not doing so won't return an error but will give us incorrect results as the Pytorch will attempt to broadcast the mishaped target tensor."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["training_targets = training_targets[:,None]\n","validation_targets = validation_targets[:,None]"]},{"cell_type":"markdown","metadata":{},"source":["### Matrix Multiplication\n","Now that we have more than one neuron (more than one row of parameters/coefficients) we need to switch from using pytorch's broadcasting to matrix multiplication so that every row of parameters is multiplied with our features. It's also worth noting using PyTorch's matrix multiplication `@` is very efficient as it's been highly optimized both algorithmically and for operating on multi-core GPUs. You'll see us using this later when we create the neural net.\n","\n","```python\n","torch.sigmoid(coefficients @ features)\n","```"]},{"cell_type":"markdown","metadata":{},"source":["### Updating multiple coefficients\n","We also need to loop through our coefficients now we have more than one layer. We simpy add a loop to our existing function."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def update_coefficients(coefficients, learning_rate):\n","    for layer in coefficients:\n","        layer.sub_(layer.grad * learning_rate)\n","        layer.grad.zero_()"]},{"cell_type":"markdown","metadata":{},"source":["### Creating the Neural net\n","Our plan for this neural net is to have two layers.\n","\n","12 Features -> |20 neuron hidden layer| --(Relu)-> |1 neuron output layer| --(Sigmoid)-> 1 Survival prediction"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(df.Name[788])\n","print(str(feature_tensor[0].shape) + \" --> \" + str(layer1.shape) + \" --> \" + str(layer2.shape) + \" --> \" + str(predictions[0].item()))"]},{"cell_type":"markdown","metadata":{},"source":["If you're curious [Bertram](https://titanic.fandom.com/wiki/Bertram_Vere_Dean) did indeed survive and lived until 1992."]},{"cell_type":"markdown","metadata":{},"source":["\n","The RELU is needed as adding together two linear functions just gives us another linear function which doesn't give us any more resolution for our calculation. Combining each linear layer with a non-linear RELU allows us to keep each linear functions utility increasing our algortihms accuracy."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch.nn.functional as F\n","\n","def create_predictions(features: Tensor, coefficients: Tuple[Tensor, Tensor, Tensor]) -> Tensor:\n","    layer1_parameters, layer2_parameters, constant_parameter = coefficients\n","    layer1_output = F.relu(features @ layer1_parameters)\n","    layer2_output = torch.sigmoid(layer1_output @ layer2_parameters + constant_parameter)\n","    return layer2_output"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["coefficients = train_model(learning_rate=20)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["calculate_accuracy(coefficients, features=validation_features)"]},{"cell_type":"markdown","metadata":{},"source":["### Linear Regression vs Neural Net\n","Our 82.58% accuracy is by (I compared my results against fast.ai's where the same thing happeneed to them) coincidence the same result as we got from the linear model. Usually a neural net will have improved performance against a linear model. However for relatively small simple datasets such as the titanic dataset linear regression will usually do just fine and less complexity isn't neeeded."]},{"cell_type":"markdown","metadata":{},"source":["## Deep Learning\n","*Note: This section does not produce the same results as fast.ai or good quality ones for that matter. There's either a mistake in my code or the fine-tuning fast.ai has applied needs me to replicate their data to the letter which isn't the case for this last section. However I'm trying to be less of a perfectionist and I think it still showcases a deep learning model so this should be fine.*\n","\n","Our neural net consisted of the input layer, one hidden layer and an output layer. For something to really be deep learning we want to have many input layers. In this last section we'll adapt our code so we can have multiple layers we can loop through. For now we'll add one additional layer."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def generate_coefficients(features: Tensor, hidden_layer_neuron_counts: List[int] = [10,10]) -> Tuple[List[Tensor], List[Tensor]]:\n","    feature_count = features.shape[1]\n","    layer_neuron_counts = [feature_count] + hidden_layer_neuron_counts + [1]\n","    layer_count = len(layer_neuron_counts)\n","    \n","    layers = []\n","    constants = []\n","    for i in range(layer_count-1):\n","        current_layer_neuron_count = layer_neuron_counts[i]\n","        next_layer_neuron_count = layer_neuron_counts[i+1]\n","        current_layer_weights = torch.rand(current_layer_neuron_count, next_layer_neuron_count) -0.3\n","        current_layer_weights = current_layer_weights / next_layer_neuron_count * 4\n","        layers.append(current_layer_weights.requires_grad_())\n","        current_layer_constant = (torch.rand(1)[0] - 0.5)/next_layer_neuron_count\n","        constants.append(current_layer_constant.requires_grad_())\n","    return layers, constants"]},{"cell_type":"markdown","metadata":{},"source":["### Fine Tuning\n","Note that the above code requires a lot of small fine tunings such as the `- 0.3` deducted from the weight initialization, and the `* 4` applied to the normalization. This is in part why deep learning didn't take off until recently, the networks can be very finnicky to get working. There are however techniques for"]},{"cell_type":"markdown","metadata":{},"source":["The creation of predictions and updating of coefficients also needs to be adjusted for having loops which we do below.\n","\n","For creating predictions we stop our loop at the last layer so we can apply a sigmoid instead of a relu function for one neuron output."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def create_predictions(features: Tensor, coefficients: Tuple[List[Tensor], List[Tensor]]) -> Tensor:\n","    result = features\n","    layers, constants = coefficients\n","    for layer, constant in zip(layers[:-1], constants[:-1]):\n","        result = torch.relu(result @ layer + constant)\n","    \n","    return torch.sigmoid(result @ layers[-1] + constants[-1])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def update_coefficients(coefficients: Tuple[List[Tensor], List[Tensor]], learning_rate: float):\n","    layers, constants = coefficients\n","    for layer in layers+constants:\n","        assert layer.grad is not None\n","        layer.sub_(layer.grad * learning_rate)\n","        layer.grad.zero_()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["coefficients = train_model(learning_rate=4)\n","calculate_accuracy(coefficients, features=validation_features)"]},{"cell_type":"markdown","metadata":{},"source":["The steps for this final deep learning model are rudimentary but do showcase what most deep learning models consist of. There are some key difference between this and more complex modern models.\n","\n","* Additional techniques for initalization and normalization of parameters\n","* Regularization (to avoid overfitting)\n","* Modifying the neural net itself to take advantage of knowledge of the problem domain.\n","* Using smaller batches for gradient descent rather than putting our entire dataset in at once.\n","\n","There topics will be addressed in future notebooks."]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"databundleVersionId":26502,"sourceId":3136,"sourceType":"competition"}],"dockerImageVersionId":30558,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
