{"cells":[{"cell_type":"code","execution_count":5,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-12-15T18:36:11.577650Z","iopub.status.busy":"2023-12-15T18:36:11.576570Z","iopub.status.idle":"2023-12-15T18:36:11.587718Z","shell.execute_reply":"2023-12-15T18:36:11.586441Z","shell.execute_reply.started":"2023-12-15T18:36:11.577565Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Running on Kaggle: False\n"]}],"source":["import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","import os\n","\n","is_kaggle = \"KAGGLE_WORKING_DIR\" in os.environ or \"/kaggle\" in os.getcwd()\n","print(\"Running on Kaggle:\", is_kaggle)"]},{"cell_type":"markdown","metadata":{},"source":["This is my attempt to reproduce the basic gradient descent used in fast.ai's ML for coder's course in Python. It's originally done in an excel spreadsheet. I don't have an excel license and the \"Solver\" functionality in excel is not available in numbers so I'm going to try and attempt it in Python with minimal use of typical ML frameworks"]},{"cell_type":"markdown","metadata":{},"source":["## Load Data set"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-12-15T18:36:11.590356Z","iopub.status.busy":"2023-12-15T18:36:11.590031Z","iopub.status.idle":"2023-12-15T18:36:11.627922Z","shell.execute_reply":"2023-12-15T18:36:11.626658Z","shell.execute_reply.started":"2023-12-15T18:36:11.590329Z"},"trusted":true},"outputs":[{"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/Users/Declan/Projects/fastai-projects/titanictrain.csv'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[6], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m      4\u001b[0m     data_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetcwd()\n\u001b[0;32m----> 6\u001b[0m training_dataframe \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m serving_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(data_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m training_dataframe\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m10\u001b[39m)\n","File \u001b[0;32m~/micromamba/envs/titanic/lib/python3.10/site-packages/pandas/io/parsers/readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    945\u001b[0m )\n\u001b[1;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/micromamba/envs/titanic/lib/python3.10/site-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n","File \u001b[0;32m~/micromamba/envs/titanic/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/micromamba/envs/titanic/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1704\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1706\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1711\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1712\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1714\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1715\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1716\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n","File \u001b[0;32m~/micromamba/envs/titanic/lib/python3.10/site-packages/pandas/io/common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    859\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    860\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    862\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/Declan/Projects/fastai-projects/titanictrain.csv'"]}],"source":["if is_kaggle:\n","    data_path = \"/kaggle/input/titanic/\"\n","else:\n","    data_path = os.getcwd() + \"/\"\n","    \n","training_dataframe = pd.read_csv(data_path + \"train.csv\")\n","serving_df = pd.read_csv(data_path + \"test.csv\")\n","\n","training_dataframe.head(10)"]},{"cell_type":"markdown","metadata":{},"source":["## Prepare Data set\n","### Data Removal\n","First we'll remove the columns that won't be useful"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-15T18:36:11.630156Z","iopub.status.busy":"2023-12-15T18:36:11.629544Z","iopub.status.idle":"2023-12-15T18:36:11.651071Z","shell.execute_reply":"2023-12-15T18:36:11.650181Z","shell.execute_reply.started":"2023-12-15T18:36:11.630123Z"},"trusted":true},"outputs":[],"source":["def remove_irrelevant_data(old_df: pd.DataFrame) -> pd.DataFrame:\n","    new_df = old_df.copy()\n","    columns_to_drop = [\"PassengerId\", \"Name\", \"Ticket\", \"Cabin\"]\n","    return new_df.drop(columns=columns_to_drop)\n","    \n","training_dataframe = remove_irrelevant_data(training_dataframe)\n","training_dataframe.head(10)"]},{"cell_type":"markdown","metadata":{},"source":["There are also some rows with empty values we should remove"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-15T18:36:11.653330Z","iopub.status.busy":"2023-12-15T18:36:11.653002Z","iopub.status.idle":"2023-12-15T18:36:11.662834Z","shell.execute_reply":"2023-12-15T18:36:11.661633Z","shell.execute_reply.started":"2023-12-15T18:36:11.653301Z"},"trusted":true},"outputs":[],"source":["def remove_na_values(old_df: pd.DataFrame) -> pd.DataFrame:\n","    cleaned_df = old_df.copy()\n","    cleaned_df = cleaned_df.dropna()\n","    removed_row_count = old_df.shape[0] - cleaned_df.shape[0]\n","    print(f\"{removed_row_count} entries were removed, {cleaned_df.shape[0]} entries remain\")\n","    return cleaned_df\n","\n","training_dataframe = remove_na_values(training_dataframe)"]},{"cell_type":"markdown","metadata":{},"source":["### Converting Category Data to Binary Categorical Values\n","Sex, the Passenger class and Embarking city are not measurable attributes so we should convert them to Boolean numbers that can be used as co-efficients"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-15T18:36:11.664903Z","iopub.status.busy":"2023-12-15T18:36:11.664551Z","iopub.status.idle":"2023-12-15T18:36:11.691977Z","shell.execute_reply":"2023-12-15T18:36:11.690666Z","shell.execute_reply.started":"2023-12-15T18:36:11.664869Z"},"trusted":true},"outputs":[],"source":["def convert_ticket_class_to_binary_values(original_df: pd.DataFrame) -> pd.DataFrame:\n","    new_df = original_df.copy()\n","    new_df[\"FirstClass\"] = new_df[\"Pclass\"].apply(lambda x: binary_equal_to_value(x,1))\n","    new_df[\"SecondClass\"] = new_df[\"Pclass\"].apply(lambda x: binary_equal_to_value(x,2))\n","    new_df.drop(\"Pclass\", axis=1, inplace=True)\n","    return new_df\n","    \n","\n","def binary_equal_to_value(number, compare_value):\n","    if (number == compare_value):\n","        return 1\n","    return 0\n","\n","training_dataframe = convert_ticket_class_to_binary_values(training_dataframe)\n","training_dataframe.head(10)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-15T18:36:11.693894Z","iopub.status.busy":"2023-12-15T18:36:11.693463Z","iopub.status.idle":"2023-12-15T18:36:11.720928Z","shell.execute_reply":"2023-12-15T18:36:11.719803Z","shell.execute_reply.started":"2023-12-15T18:36:11.693848Z"},"trusted":true},"outputs":[],"source":["def convert_embarkation_port_to_binary_values(old_df: pd.DataFrame) -> pd.DataFrame:\n","    new_df = old_df.copy()\n","    new_df[\"Cherbourg_Departure\"] = old_df[\"Embarked\"].apply(lambda x: binary_equal_to_value(x, 'C'))\n","    new_df[\"Queenstown_Departure\"] = old_df[\"Embarked\"].apply(lambda x: binary_equal_to_value(x, 'Q'))\n","    new_df.drop(\"Embarked\", axis=1, inplace=True)\n","    return new_df\n","    \n","\n","training_dataframe = convert_embarkation_port_to_binary_values(training_dataframe)\n","training_dataframe.head(10)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-15T18:36:11.722999Z","iopub.status.busy":"2023-12-15T18:36:11.722575Z","iopub.status.idle":"2023-12-15T18:36:11.744750Z","shell.execute_reply":"2023-12-15T18:36:11.743646Z","shell.execute_reply.started":"2023-12-15T18:36:11.722961Z"},"trusted":true},"outputs":[],"source":["def convert_sex_to_binary_value(old_df: pd.DataFrame) -> pd.DataFrame:\n","    new_df = old_df.copy()\n","    new_df[\"Sex\"] = old_df[\"Sex\"].apply(lambda x: binary_equal_to_value(x, \"male\"))\n","    return new_df\n","\n","training_dataframe = convert_sex_to_binary_value(training_dataframe)\n","training_dataframe.head(10)"]},{"cell_type":"markdown","metadata":{},"source":["### Converting numbers to fractional values\n","#### Age\n","Larger numbers would have too great an impact on our calculations so we can normalize them by dividing them by their max value them so they're between 0 and 1"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-15T18:36:11.746963Z","iopub.status.busy":"2023-12-15T18:36:11.746619Z","iopub.status.idle":"2023-12-15T18:36:11.769574Z","shell.execute_reply":"2023-12-15T18:36:11.768464Z","shell.execute_reply.started":"2023-12-15T18:36:11.746934Z"},"trusted":true},"outputs":[],"source":["def convert_numeric_column_to_decimal(old_df: pd.DataFrame, column_name: str) -> pd.DataFrame:\n","    new_df = old_df.copy()\n","    max_numeric_value = old_df[column_name].max()\n","    new_df[column_name] = old_df[column_name].apply(lambda x: x/max_numeric_value)\n","    return new_df\n","    \n","training_dataframe = convert_numeric_column_to_decimal(training_dataframe, \"Age\")\n","training_dataframe.head(10)"]},{"cell_type":"markdown","metadata":{},"source":["#### Fare\n","The `Fare` column has lots of small values with the occasional very large value. Uniform normalization using the max value isn't ideal when we're dealing with lots of small values with occasional very large values as the variation between the lower numbers will be lost. To normalize the values we can use a log function (log10 here) to bring the numbers down to reasonable ranges. We must use `log10(x+1)` to avoid 0 values as `log10(0)` would give us infinity."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-15T18:36:11.771533Z","iopub.status.busy":"2023-12-15T18:36:11.771187Z","iopub.status.idle":"2023-12-15T18:36:11.795392Z","shell.execute_reply":"2023-12-15T18:36:11.794544Z","shell.execute_reply.started":"2023-12-15T18:36:11.771504Z"},"trusted":true},"outputs":[],"source":["import math\n","def convert_numeric_column_to_decimal_with_logarithm(old_df: pd.DataFrame, column_name: str) -> pd.DataFrame:\n","    new_df = old_df.copy()\n","    new_df[column_name] = new_df[column_name].apply(lambda x: math.log10(x+1) if x > 0 else 0)\n","    new_df = convert_numeric_column_to_decimal(new_df, column_name)\n","    return new_df\n","\n","training_dataframe = convert_numeric_column_to_decimal_with_logarithm(training_dataframe, \"Fare\")\n","training_dataframe.head(10)"]},{"cell_type":"markdown","metadata":{},"source":["## Linear Regression\n","### Add a constant value\n","A linear function needs a constant, this will be needed for the maths so we should add a column full of ones"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-15T18:36:11.800280Z","iopub.status.busy":"2023-12-15T18:36:11.799914Z","iopub.status.idle":"2023-12-15T18:36:11.818268Z","shell.execute_reply":"2023-12-15T18:36:11.816933Z","shell.execute_reply.started":"2023-12-15T18:36:11.800248Z"},"trusted":true},"outputs":[],"source":["training_dataframe[\"Constant\"] = 1\n","training_dataframe.head(10)"]},{"cell_type":"markdown","metadata":{},"source":["### Prepare initial linear co-efficient values\n","We want to set each of our parameter values to a random number close to 1. The survived column is not a parameter but our desired result/output so we don't include this."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-15T18:36:11.820322Z","iopub.status.busy":"2023-12-15T18:36:11.819991Z","iopub.status.idle":"2023-12-15T18:36:11.833908Z","shell.execute_reply":"2023-12-15T18:36:11.832743Z","shell.execute_reply.started":"2023-12-15T18:36:11.820295Z"},"trusted":true},"outputs":[],"source":["input_df = training_dataframe.drop(\"Survived\", axis=1)\n","linear_parameters = np.random.rand(input_df.shape[1]).tolist()\n","linear_parameters"]},{"cell_type":"markdown","metadata":{},"source":["### Calculate the linear function of our parameters multiplied by our random Coefficients"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-15T18:36:11.836343Z","iopub.status.busy":"2023-12-15T18:36:11.835997Z","iopub.status.idle":"2023-12-15T18:36:11.858845Z","shell.execute_reply":"2023-12-15T18:36:11.857637Z","shell.execute_reply.started":"2023-12-15T18:36:11.836315Z"},"trusted":true},"outputs":[],"source":["def calculate_linear_result() -> np.array:\n","    return input_df.apply(lambda row: row.dot(linear_parameters), axis=1).to_numpy()\n","\n","training_dataframe[\"Initial Linear Result\"] = input_df.apply(lambda row: row.dot(linear_parameters), axis=1)"]},{"cell_type":"markdown","metadata":{},"source":["### Gradient Descent"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-15T18:36:11.860841Z","iopub.status.busy":"2023-12-15T18:36:11.860409Z","iopub.status.idle":"2023-12-15T18:36:22.655866Z","shell.execute_reply":"2023-12-15T18:36:22.654577Z","shell.execute_reply.started":"2023-12-15T18:36:11.860799Z"},"trusted":true},"outputs":[],"source":["def optimize_weights(inputs: [pd.DataFrame], target_variables: np.array, parameters: [float], learning_rate: float=0.01, epochs: int=1000) -> [float]:\n","    for current_epoch in range(epochs):\n","        # Predicted values\n","        predicted_values = inputs.apply(lambda row: row.dot(parameters), axis=1).to_numpy()\n","    \n","        # Calculate error\n","        errors = predicted_values - target_variables\n","        mean_square_error = (errors ** 2).mean()\n","    \n","        if current_epoch % 100 == 0: #Print every 100th value\n","            print(mean_square_error)\n","    \n","        # Calculate gradient\n","        gradient = np.dot(inputs.to_numpy().T, errors) * 2 / len(target_variables)\n","    \n","        # Update parameters\n","        parameters -= learning_rate * gradient\n","    # Final parameters\n","    print(f\"Optimized weights: {parameters}\")\n","    print(f\"Final error: {mean_square_error}\")\n","    return parameters\n","    \n","linear_parameters = optimize_weights(inputs=input_df, target_variables=training_dataframe[\"Survived\"].to_numpy(), parameters=linear_parameters)"]},{"cell_type":"markdown","metadata":{},"source":["## Neural Nets\n","The calculation above was a linear regression as we only use one set of parameters.\n","Here we'll use two sets of parameters, apply a RELU (Rectified Linear Unit) function and add them together to give us a loss. A RELU function is non-linear and simply replaces every negative number with a 0.\n","\n","The RELU is needed as adding together two linear functions just gives us another linear function which doesn't give us any more resolution for our calculation. Combinging each linear layer with a non-linear RELU allows us to keep each linear functions utility increasing our algortihms accuracy."]},{"cell_type":"markdown","metadata":{},"source":["### Create Matrix of Relu Values"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-15T18:36:22.660525Z","iopub.status.busy":"2023-12-15T18:36:22.659415Z","iopub.status.idle":"2023-12-15T18:36:22.666015Z","shell.execute_reply":"2023-12-15T18:36:22.664952Z","shell.execute_reply.started":"2023-12-15T18:36:22.660473Z"},"trusted":true},"outputs":[],"source":["np.random.seed(42)\n","parameter_matrix = np.random.rand(2, input_df.shape[1]) - 0.5\n","known_survival_matrix = training_dataframe[\"Survived\"].to_numpy().reshape(-1,1)\n","inputs = input_df.to_numpy()"]},{"cell_type":"markdown","metadata":{},"source":["### Relu Gradient Descent (non-linear)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-15T18:36:22.667248Z","iopub.status.busy":"2023-12-15T18:36:22.666958Z","iopub.status.idle":"2023-12-15T18:36:22.786974Z","shell.execute_reply":"2023-12-15T18:36:22.785761Z","shell.execute_reply.started":"2023-12-15T18:36:22.667223Z"},"trusted":true},"outputs":[],"source":["# Gradient descent\n","for current_epoch in range(1000):\n","    # Predicted values\n","    predicted_value_matrix = np.dot(inputs, parameter_matrix.T)\n","    relu_value_matrix = np.maximum(predicted_value_matrix, 0)\n","    \n","    # Calculate error\n","    errors = relu_value_matrix - known_survival_matrix\n","    summed_errors = np.sum(errors, axis=1)\n","    if current_epoch % 100 == 0: #Print every 100th value\n","        print(summed_errors.mean())\n","    \n","    # Calculate gradient\n","    gradient = np.dot(inputs.T, summed_errors) * 2 / len(training_dataframe[\"Survived\"].to_numpy())\n","    \n","    # Update parameters\n","    parameter_matrix -= 0.01 * gradient\n","    nn_params = parameter_matrix.sum(axis=0)\n","\n","# Final parameters\n","print(f\"Optimized weights: {nn_params}\")"]},{"cell_type":"markdown","metadata":{},"source":["## Create Titanic survial predictions"]},{"cell_type":"markdown","metadata":{},"source":["Now we'll use the parameters we've calculated to try and make predictions about the survivors in our validation set."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-15T18:36:22.788466Z","iopub.status.busy":"2023-12-15T18:36:22.788155Z","iopub.status.idle":"2023-12-15T18:36:22.814972Z","shell.execute_reply":"2023-12-15T18:36:22.813815Z","shell.execute_reply.started":"2023-12-15T18:36:22.788438Z"},"trusted":true},"outputs":[],"source":["serving_df"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-15T18:36:22.819159Z","iopub.status.busy":"2023-12-15T18:36:22.816741Z","iopub.status.idle":"2023-12-15T18:36:22.864182Z","shell.execute_reply":"2023-12-15T18:36:22.862846Z","shell.execute_reply.started":"2023-12-15T18:36:22.819120Z"},"trusted":true},"outputs":[],"source":["def estimate_missing_ages(old_df: pd.DataFrame) -> pd.DataFrame:\n","    new_df = old_df.copy()\n","    mean_age = old_df[\"Age\"].mean()\n","    new_df[\"Age\"].fillna(value=mean_age, inplace=True)\n","    return new_df\n","\n","def estimate_missing_fares(old_df: pd.DataFrame) -> pd.DataFrame:\n","    new_df = old_df.copy()\n","    new_df[\"Fare\"].fillna(value=0, inplace=True)\n","    return new_df\n","    \n","def prepare_data(old_df: pd.DataFrame) -> pd.DataFrame:\n","    new_df = old_df.copy()\n","    new_df = remove_irrelevant_data(new_df)\n","    new_df = estimate_missing_ages(new_df)\n","    new_df = estimate_missing_fares(new_df)\n","    print(\"Searching for NA values:\")\n","    print(new_df.isna().any())\n","    new_df = convert_ticket_class_to_binary_values(new_df)\n","    new_df = convert_embarkation_port_to_binary_values(new_df)\n","    new_df = convert_sex_to_binary_value(new_df)\n","    new_df = convert_numeric_column_to_decimal(new_df, \"Age\")\n","    new_df = convert_numeric_column_to_decimal_with_logarithm(new_df, \"Fare\")\n","    new_df[\"Constant\"] = 1\n","    return new_df\n","    \n","serving_df = prepare_data(serving_df)\n","assert (input_df.columns == serving_df.columns).all()\n","serving_df"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-15T18:36:22.866861Z","iopub.status.busy":"2023-12-15T18:36:22.866374Z","iopub.status.idle":"2023-12-15T18:36:22.890775Z","shell.execute_reply":"2023-12-15T18:36:22.889549Z","shell.execute_reply.started":"2023-12-15T18:36:22.866818Z"},"trusted":true},"outputs":[],"source":["def create_predictions(validation_df: pd.DataFrame, optimized_weights: np.array) -> np.array:\n","    return np.dot(validation_df.to_numpy(), optimized_weights)\n","\n","serving_df[\"Survival Prediction\"] = create_predictions(serving_df, nn_params)\n","serving_df"]},{"cell_type":"markdown","metadata":{},"source":["## Prepare Submission CSV"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-15T18:36:22.892540Z","iopub.status.busy":"2023-12-15T18:36:22.892207Z","iopub.status.idle":"2023-12-15T18:36:22.919481Z","shell.execute_reply":"2023-12-15T18:36:22.918559Z","shell.execute_reply.started":"2023-12-15T18:36:22.892511Z"},"trusted":true},"outputs":[],"source":["original_validation_df = pd.read_csv(data_path + \"test.csv\")\n","submission_df = pd.DataFrame()\n","submission_df[\"PassengerId\"] = original_validation_df[\"PassengerId\"]\n","submission_df[\"Survived\"] = serving_df[\"Survival Prediction\"].apply(lambda x: 0 if x < 0.5 else 1)\n","submission_df.to_csv(\"submission.csv\", index=False)\n","submission_df"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"databundleVersionId":26502,"sourceId":3136,"sourceType":"competition"}],"dockerImageVersionId":30558,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"}},"nbformat":4,"nbformat_minor":4}
