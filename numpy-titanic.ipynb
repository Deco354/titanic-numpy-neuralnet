{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3136,"databundleVersionId":26502,"sourceType":"competition"}],"dockerImageVersionId":30558,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-15T18:36:11.576570Z","iopub.execute_input":"2023-12-15T18:36:11.577650Z","iopub.status.idle":"2023-12-15T18:36:11.587718Z","shell.execute_reply.started":"2023-12-15T18:36:11.577565Z","shell.execute_reply":"2023-12-15T18:36:11.586441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is my attempt to reproduce the basic gradient descent used in fast.ai's ML for coder's course in Python. It's originally done in an excel spreadsheet. I don't have an excel license and the \"Solver\" functionality in excel is not available in numbers so I'm going to try and attempt it in Python with minimal use of typical ML frameworks","metadata":{}},{"cell_type":"markdown","source":"## Load Data set","metadata":{}},{"cell_type":"code","source":"training_dataframe = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\nserving_df = pd.read_csv(\"/kaggle/input/titanic/test.csv\")\n\ntraining_dataframe.head(10)","metadata":{"execution":{"iopub.status.busy":"2023-12-15T18:36:11.590031Z","iopub.execute_input":"2023-12-15T18:36:11.590356Z","iopub.status.idle":"2023-12-15T18:36:11.627922Z","shell.execute_reply.started":"2023-12-15T18:36:11.590329Z","shell.execute_reply":"2023-12-15T18:36:11.626658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Prepare Data set\n### Data Removal\nFirst we'll remove the columns that won't be useful","metadata":{}},{"cell_type":"code","source":"def remove_irrelevant_data(old_df: pd.DataFrame) -> pd.DataFrame:\n    new_df = old_df.copy()\n    columns_to_drop = [\"PassengerId\", \"Name\", \"Ticket\", \"Cabin\"]\n    return new_df.drop(columns=columns_to_drop)\n    \ntraining_dataframe = remove_irrelevant_data(training_dataframe)\ntraining_dataframe.head(10)","metadata":{"execution":{"iopub.status.busy":"2023-12-15T18:36:11.629544Z","iopub.execute_input":"2023-12-15T18:36:11.630156Z","iopub.status.idle":"2023-12-15T18:36:11.651071Z","shell.execute_reply.started":"2023-12-15T18:36:11.630123Z","shell.execute_reply":"2023-12-15T18:36:11.650181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are also some rows with empty values we should remove","metadata":{}},{"cell_type":"code","source":"def remove_na_values(old_df: pd.DataFrame) -> pd.DataFrame:\n    cleaned_df = old_df.copy()\n    cleaned_df = cleaned_df.dropna()\n    removed_row_count = old_df.shape[0] - cleaned_df.shape[0]\n    print(f\"{removed_row_count} entries were removed, {cleaned_df.shape[0]} entries remain\")\n    return cleaned_df\n\ntraining_dataframe = remove_na_values(training_dataframe)","metadata":{"execution":{"iopub.status.busy":"2023-12-15T18:36:11.653002Z","iopub.execute_input":"2023-12-15T18:36:11.653330Z","iopub.status.idle":"2023-12-15T18:36:11.662834Z","shell.execute_reply.started":"2023-12-15T18:36:11.653301Z","shell.execute_reply":"2023-12-15T18:36:11.661633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Converting Category Data to Binary Categorical Values\nSex, the Passenger class and Embarking city are not measurable attributes so we should convert them to Boolean numbers that can be used as co-efficients","metadata":{}},{"cell_type":"code","source":"def convert_ticket_class_to_binary_values(original_df: pd.DataFrame) -> pd.DataFrame:\n    new_df = original_df.copy()\n    new_df[\"FirstClass\"] = new_df[\"Pclass\"].apply(lambda x: binary_equal_to_value(x,1))\n    new_df[\"SecondClass\"] = new_df[\"Pclass\"].apply(lambda x: binary_equal_to_value(x,2))\n    new_df.drop(\"Pclass\", axis=1, inplace=True)\n    return new_df\n    \n\ndef binary_equal_to_value(number, compare_value):\n    if (number == compare_value):\n        return 1\n    return 0\n\ntraining_dataframe = convert_ticket_class_to_binary_values(training_dataframe)\ntraining_dataframe.head(10)","metadata":{"execution":{"iopub.status.busy":"2023-12-15T18:36:11.664551Z","iopub.execute_input":"2023-12-15T18:36:11.664903Z","iopub.status.idle":"2023-12-15T18:36:11.691977Z","shell.execute_reply.started":"2023-12-15T18:36:11.664869Z","shell.execute_reply":"2023-12-15T18:36:11.690666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def convert_embarkation_port_to_binary_values(old_df: pd.DataFrame) -> pd.DataFrame:\n    new_df = old_df.copy()\n    new_df[\"Cherbourg_Departure\"] = old_df[\"Embarked\"].apply(lambda x: binary_equal_to_value(x, 'C'))\n    new_df[\"Queenstown_Departure\"] = old_df[\"Embarked\"].apply(lambda x: binary_equal_to_value(x, 'Q'))\n    new_df.drop(\"Embarked\", axis=1, inplace=True)\n    return new_df\n    \n\ntraining_dataframe = convert_embarkation_port_to_binary_values(training_dataframe)\ntraining_dataframe.head(10)","metadata":{"execution":{"iopub.status.busy":"2023-12-15T18:36:11.693463Z","iopub.execute_input":"2023-12-15T18:36:11.693894Z","iopub.status.idle":"2023-12-15T18:36:11.720928Z","shell.execute_reply.started":"2023-12-15T18:36:11.693848Z","shell.execute_reply":"2023-12-15T18:36:11.719803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def convert_sex_to_binary_value(old_df: pd.DataFrame) -> pd.DataFrame:\n    new_df = old_df.copy()\n    new_df[\"Sex\"] = old_df[\"Sex\"].apply(lambda x: binary_equal_to_value(x, \"male\"))\n    return new_df\n\ntraining_dataframe = convert_sex_to_binary_value(training_dataframe)\ntraining_dataframe.head(10)","metadata":{"execution":{"iopub.status.busy":"2023-12-15T18:36:11.722575Z","iopub.execute_input":"2023-12-15T18:36:11.722999Z","iopub.status.idle":"2023-12-15T18:36:11.744750Z","shell.execute_reply.started":"2023-12-15T18:36:11.722961Z","shell.execute_reply":"2023-12-15T18:36:11.743646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Converting numbers to fractional values\n#### Age\nLarger numbers would have too great an impact on our calculations so we can normalize them by dividing them by their max value them so they're between 0 and 1","metadata":{}},{"cell_type":"code","source":"def convert_numeric_column_to_decimal(old_df: pd.DataFrame, column_name: str) -> pd.DataFrame:\n    new_df = old_df.copy()\n    max_numeric_value = old_df[column_name].max()\n    new_df[column_name] = old_df[column_name].apply(lambda x: x/max_numeric_value)\n    return new_df\n    \ntraining_dataframe = convert_numeric_column_to_decimal(training_dataframe, \"Age\")\ntraining_dataframe.head(10)","metadata":{"execution":{"iopub.status.busy":"2023-12-15T18:36:11.746619Z","iopub.execute_input":"2023-12-15T18:36:11.746963Z","iopub.status.idle":"2023-12-15T18:36:11.769574Z","shell.execute_reply.started":"2023-12-15T18:36:11.746934Z","shell.execute_reply":"2023-12-15T18:36:11.768464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Fare\nThe `Fare` column has lots of small values with the occasional very large value. Uniform normalization using the max value isn't ideal when we're dealing with lots of small values with occasional very large values as the variation between the lower numbers will be lost. To normalize the values we can use a log function (log10 here) to bring the numbers down to reasonable ranges. We must use `log10(x+1)` to avoid 0 values as `log10(0)` would give us infinity.","metadata":{}},{"cell_type":"code","source":"import math\ndef convert_numeric_column_to_decimal_with_logarithm(old_df: pd.DataFrame, column_name: str) -> pd.DataFrame:\n    new_df = old_df.copy()\n    new_df[column_name] = new_df[column_name].apply(lambda x: math.log10(x+1) if x > 0 else 0)\n    new_df = convert_numeric_column_to_decimal(new_df, column_name)\n    return new_df\n\ntraining_dataframe = convert_numeric_column_to_decimal_with_logarithm(training_dataframe, \"Fare\")\ntraining_dataframe.head(10)","metadata":{"execution":{"iopub.status.busy":"2023-12-15T18:36:11.771187Z","iopub.execute_input":"2023-12-15T18:36:11.771533Z","iopub.status.idle":"2023-12-15T18:36:11.795392Z","shell.execute_reply.started":"2023-12-15T18:36:11.771504Z","shell.execute_reply":"2023-12-15T18:36:11.794544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Linear Regression\n### Add a constant value\nA linear function needs a constant, this will be needed for the maths so we should add a column full of ones","metadata":{}},{"cell_type":"code","source":"training_dataframe[\"Constant\"] = 1\ntraining_dataframe.head(10)","metadata":{"execution":{"iopub.status.busy":"2023-12-15T18:36:11.799914Z","iopub.execute_input":"2023-12-15T18:36:11.800280Z","iopub.status.idle":"2023-12-15T18:36:11.818268Z","shell.execute_reply.started":"2023-12-15T18:36:11.800248Z","shell.execute_reply":"2023-12-15T18:36:11.816933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Prepare initial linear co-efficient values\nWe want to set each of our parameter values to a random number close to 1. The survived column is not a parameter but our desired result/output so we don't include this.","metadata":{}},{"cell_type":"code","source":"input_df = training_dataframe.drop(\"Survived\", axis=1)\nlinear_parameters = np.random.rand(input_df.shape[1]).tolist()\nlinear_parameters","metadata":{"execution":{"iopub.status.busy":"2023-12-15T18:36:11.819991Z","iopub.execute_input":"2023-12-15T18:36:11.820322Z","iopub.status.idle":"2023-12-15T18:36:11.833908Z","shell.execute_reply.started":"2023-12-15T18:36:11.820295Z","shell.execute_reply":"2023-12-15T18:36:11.832743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Calculate the linear function of our parameters multiplied by our random Coefficients","metadata":{}},{"cell_type":"code","source":"def calculate_linear_result() -> np.array:\n    return input_df.apply(lambda row: row.dot(linear_parameters), axis=1).to_numpy()\n\ntraining_dataframe[\"Initial Linear Result\"] = input_df.apply(lambda row: row.dot(linear_parameters), axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-12-15T18:36:11.835997Z","iopub.execute_input":"2023-12-15T18:36:11.836343Z","iopub.status.idle":"2023-12-15T18:36:11.858845Z","shell.execute_reply.started":"2023-12-15T18:36:11.836315Z","shell.execute_reply":"2023-12-15T18:36:11.857637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Gradient Descent","metadata":{}},{"cell_type":"code","source":"def optimize_weights(inputs: [pd.DataFrame], target_variables: np.array, parameters: [float], learning_rate: float=0.01, epochs: int=1000) -> [float]:\n    for current_epoch in range(epochs):\n        # Predicted values\n        predicted_values = inputs.apply(lambda row: row.dot(parameters), axis=1).to_numpy()\n    \n        # Calculate error\n        errors = predicted_values - target_variables\n        mean_square_error = (errors ** 2).mean()\n    \n        if current_epoch % 100 == 0: #Print every 100th value\n            print(mean_square_error)\n    \n        # Calculate gradient\n        gradient = np.dot(inputs.to_numpy().T, errors) * 2 / len(target_variables)\n    \n        # Update parameters\n        parameters -= learning_rate * gradient\n    # Final parameters\n    print(f\"Optimized weights: {parameters}\")\n    print(f\"Final error: {mean_square_error}\")\n    return parameters\n    \nlinear_parameters = optimize_weights(inputs=input_df, target_variables=training_dataframe[\"Survived\"].to_numpy(), parameters=linear_parameters)","metadata":{"execution":{"iopub.status.busy":"2023-12-15T18:36:11.860409Z","iopub.execute_input":"2023-12-15T18:36:11.860841Z","iopub.status.idle":"2023-12-15T18:36:22.655866Z","shell.execute_reply.started":"2023-12-15T18:36:11.860799Z","shell.execute_reply":"2023-12-15T18:36:22.654577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Neural Nets\nThe calculation above was a linear regression as we only use one set of parameters.\nHere we'll use two sets of parameters, apply a RELU (Rectified Linear Unit) function and add them together to give us a loss. A RELU function is non-linear and simply replaces every negative number with a 0.\n\nThe RELU is needed as adding together two linear functions just gives us another linear function which doesn't give us any more resolution for our calculation. Combinging each linear layer with a non-linear RELU allows us to keep each linear functions utility increasing our algortihms accuracy.","metadata":{}},{"cell_type":"markdown","source":"### Create Matrix of Relu Values","metadata":{}},{"cell_type":"code","source":"np.random.seed(42)\nparameter_matrix = np.random.rand(2, input_df.shape[1]) - 0.5\nknown_survival_matrix = training_dataframe[\"Survived\"].to_numpy().reshape(-1,1)\ninputs = input_df.to_numpy()","metadata":{"execution":{"iopub.status.busy":"2023-12-15T18:36:22.659415Z","iopub.execute_input":"2023-12-15T18:36:22.660525Z","iopub.status.idle":"2023-12-15T18:36:22.666015Z","shell.execute_reply.started":"2023-12-15T18:36:22.660473Z","shell.execute_reply":"2023-12-15T18:36:22.664952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Relu Gradient Descent (non-linear)","metadata":{}},{"cell_type":"code","source":"# Gradient descent\nfor current_epoch in range(1000):\n    # Predicted values\n    predicted_value_matrix = np.dot(inputs, parameter_matrix.T)\n    relu_value_matrix = np.maximum(predicted_value_matrix, 0)\n    \n    # Calculate error\n    errors = relu_value_matrix - known_survival_matrix\n    summed_errors = np.sum(errors, axis=1)\n    if current_epoch % 100 == 0: #Print every 100th value\n        print(summed_errors.mean())\n    \n    # Calculate gradient\n    gradient = np.dot(inputs.T, summed_errors) * 2 / len(training_dataframe[\"Survived\"].to_numpy())\n    \n    # Update parameters\n    parameter_matrix -= 0.01 * gradient\n    nn_params = parameter_matrix.sum(axis=0)\n\n# Final parameters\nprint(f\"Optimized weights: {nn_params}\")","metadata":{"execution":{"iopub.status.busy":"2023-12-15T18:36:22.666958Z","iopub.execute_input":"2023-12-15T18:36:22.667248Z","iopub.status.idle":"2023-12-15T18:36:22.786974Z","shell.execute_reply.started":"2023-12-15T18:36:22.667223Z","shell.execute_reply":"2023-12-15T18:36:22.785761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create Titanic survial predictions","metadata":{}},{"cell_type":"markdown","source":"Now we'll use the parameters we've calculated to try and make predictions about the survivors in our validation set.","metadata":{}},{"cell_type":"code","source":"serving_df","metadata":{"execution":{"iopub.status.busy":"2023-12-15T18:36:22.788155Z","iopub.execute_input":"2023-12-15T18:36:22.788466Z","iopub.status.idle":"2023-12-15T18:36:22.814972Z","shell.execute_reply.started":"2023-12-15T18:36:22.788438Z","shell.execute_reply":"2023-12-15T18:36:22.813815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def estimate_missing_ages(old_df: pd.DataFrame) -> pd.DataFrame:\n    new_df = old_df.copy()\n    mean_age = old_df[\"Age\"].mean()\n    new_df[\"Age\"].fillna(value=mean_age, inplace=True)\n    return new_df\n\ndef estimate_missing_fares(old_df: pd.DataFrame) -> pd.DataFrame:\n    new_df = old_df.copy()\n    new_df[\"Fare\"].fillna(value=0, inplace=True)\n    return new_df\n    \ndef prepare_data(old_df: pd.DataFrame) -> pd.DataFrame:\n    new_df = old_df.copy()\n    new_df = remove_irrelevant_data(new_df)\n    new_df = estimate_missing_ages(new_df)\n    new_df = estimate_missing_fares(new_df)\n    print(\"Searching for NA values:\")\n    print(new_df.isna().any())\n    new_df = convert_ticket_class_to_binary_values(new_df)\n    new_df = convert_embarkation_port_to_binary_values(new_df)\n    new_df = convert_sex_to_binary_value(new_df)\n    new_df = convert_numeric_column_to_decimal(new_df, \"Age\")\n    new_df = convert_numeric_column_to_decimal_with_logarithm(new_df, \"Fare\")\n    new_df[\"Constant\"] = 1\n    return new_df\n    \nserving_df = prepare_data(serving_df)\nassert (input_df.columns == serving_df.columns).all()\nserving_df","metadata":{"execution":{"iopub.status.busy":"2023-12-15T18:36:22.816741Z","iopub.execute_input":"2023-12-15T18:36:22.819159Z","iopub.status.idle":"2023-12-15T18:36:22.864182Z","shell.execute_reply.started":"2023-12-15T18:36:22.819120Z","shell.execute_reply":"2023-12-15T18:36:22.862846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_predictions(validation_df: pd.DataFrame, optimized_weights: np.array) -> np.array:\n    return np.dot(validation_df.to_numpy(), optimized_weights)\n\nserving_df[\"Survival Prediction\"] = create_predictions(serving_df, nn_params)\nserving_df","metadata":{"execution":{"iopub.status.busy":"2023-12-15T18:36:22.866374Z","iopub.execute_input":"2023-12-15T18:36:22.866861Z","iopub.status.idle":"2023-12-15T18:36:22.890775Z","shell.execute_reply.started":"2023-12-15T18:36:22.866818Z","shell.execute_reply":"2023-12-15T18:36:22.889549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Prepare Submission CSV","metadata":{}},{"cell_type":"code","source":"original_validation_df = pd.read_csv(\"/kaggle/input/titanic/test.csv\")\nsubmission_df = pd.DataFrame()\nsubmission_df[\"PassengerId\"] = original_validation_df[\"PassengerId\"]\nsubmission_df[\"Survived\"] = serving_df[\"Survival Prediction\"].apply(lambda x: 0 if x < 0.5 else 1)\nsubmission_df.to_csv(\"submission.csv\", index=False)\nsubmission_df","metadata":{"execution":{"iopub.status.busy":"2023-12-15T18:36:22.892207Z","iopub.execute_input":"2023-12-15T18:36:22.892540Z","iopub.status.idle":"2023-12-15T18:36:22.919481Z","shell.execute_reply.started":"2023-12-15T18:36:22.892511Z","shell.execute_reply":"2023-12-15T18:36:22.918559Z"},"trusted":true},"execution_count":null,"outputs":[]}]}