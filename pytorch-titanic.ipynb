{"cells":[{"cell_type":"code","execution_count":127,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-12-15T18:36:11.577650Z","iopub.status.busy":"2023-12-15T18:36:11.576570Z","iopub.status.idle":"2023-12-15T18:36:11.587718Z","shell.execute_reply":"2023-12-15T18:36:11.586441Z","shell.execute_reply.started":"2023-12-15T18:36:11.577565Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Running on Kaggle: False\n"]}],"source":["import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","import os\n","\n","is_kaggle = \"KAGGLE_WORKING_DIR\" in os.environ or \"/kaggle\" in os.getcwd()\n","print(\"Running on Kaggle:\", is_kaggle)\n","\n","if is_kaggle:\n","    data_path = \"/kaggle/input/titanic/\"\n","else:\n","    data_path = os.getcwd() + \"/\""]},{"cell_type":"code","execution_count":128,"metadata":{},"outputs":[],"source":["import torch\n","np.set_printoptions(linewidth=140)\n","torch.set_printoptions(linewidth=140, sci_mode=False, edgeitems=7)\n","pd.set_option('display.width', 140)"]},{"cell_type":"markdown","metadata":{},"source":["Based on fast.ai chapter 5 we'll now iterate on the numpy-titanic notebook by using pytorch and applying some best practices from that chapter"]},{"cell_type":"markdown","metadata":{},"source":["## Prepare Data set"]},{"cell_type":"code","execution_count":129,"metadata":{"execution":{"iopub.execute_input":"2023-12-15T18:36:11.590356Z","iopub.status.busy":"2023-12-15T18:36:11.590031Z","iopub.status.idle":"2023-12-15T18:36:11.627922Z","shell.execute_reply":"2023-12-15T18:36:11.626658Z","shell.execute_reply.started":"2023-12-15T18:36:11.590329Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>PassengerId</th>\n","      <th>Survived</th>\n","      <th>Pclass</th>\n","      <th>Name</th>\n","      <th>Sex</th>\n","      <th>Age</th>\n","      <th>SibSp</th>\n","      <th>Parch</th>\n","      <th>Ticket</th>\n","      <th>Fare</th>\n","      <th>Cabin</th>\n","      <th>Embarked</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>Braund, Mr. Owen Harris</td>\n","      <td>male</td>\n","      <td>22.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>A/5 21171</td>\n","      <td>7.2500</td>\n","      <td>NaN</td>\n","      <td>S</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>Cumings, Mrs. John Bradley (Florence Briggs Thayer)</td>\n","      <td>female</td>\n","      <td>38.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>PC 17599</td>\n","      <td>71.2833</td>\n","      <td>C85</td>\n","      <td>C</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>Heikkinen, Miss. Laina</td>\n","      <td>female</td>\n","      <td>26.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>STON/O2. 3101282</td>\n","      <td>7.9250</td>\n","      <td>NaN</td>\n","      <td>S</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n","      <td>female</td>\n","      <td>35.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>113803</td>\n","      <td>53.1000</td>\n","      <td>C123</td>\n","      <td>S</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>Allen, Mr. William Henry</td>\n","      <td>male</td>\n","      <td>35.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>373450</td>\n","      <td>8.0500</td>\n","      <td>NaN</td>\n","      <td>S</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>886</th>\n","      <td>887</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>Montvila, Rev. Juozas</td>\n","      <td>male</td>\n","      <td>27.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>211536</td>\n","      <td>13.0000</td>\n","      <td>NaN</td>\n","      <td>S</td>\n","    </tr>\n","    <tr>\n","      <th>887</th>\n","      <td>888</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>Graham, Miss. Margaret Edith</td>\n","      <td>female</td>\n","      <td>19.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>112053</td>\n","      <td>30.0000</td>\n","      <td>B42</td>\n","      <td>S</td>\n","    </tr>\n","    <tr>\n","      <th>888</th>\n","      <td>889</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>Johnston, Miss. Catherine Helen \"Carrie\"</td>\n","      <td>female</td>\n","      <td>NaN</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>W./C. 6607</td>\n","      <td>23.4500</td>\n","      <td>NaN</td>\n","      <td>S</td>\n","    </tr>\n","    <tr>\n","      <th>889</th>\n","      <td>890</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>Behr, Mr. Karl Howell</td>\n","      <td>male</td>\n","      <td>26.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>111369</td>\n","      <td>30.0000</td>\n","      <td>C148</td>\n","      <td>C</td>\n","    </tr>\n","    <tr>\n","      <th>890</th>\n","      <td>891</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>Dooley, Mr. Patrick</td>\n","      <td>male</td>\n","      <td>32.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>370376</td>\n","      <td>7.7500</td>\n","      <td>NaN</td>\n","      <td>Q</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>891 rows Ã— 12 columns</p>\n","</div>"],"text/plain":["     PassengerId  Survived  Pclass                                                 Name     Sex   Age  SibSp  Parch            Ticket  \\\n","0              1         0       3                              Braund, Mr. Owen Harris    male  22.0      1      0         A/5 21171   \n","1              2         1       1  Cumings, Mrs. John Bradley (Florence Briggs Thayer)  female  38.0      1      0          PC 17599   \n","2              3         1       3                               Heikkinen, Miss. Laina  female  26.0      0      0  STON/O2. 3101282   \n","3              4         1       1         Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1      0            113803   \n","4              5         0       3                             Allen, Mr. William Henry    male  35.0      0      0            373450   \n","..           ...       ...     ...                                                  ...     ...   ...    ...    ...               ...   \n","886          887         0       2                                Montvila, Rev. Juozas    male  27.0      0      0            211536   \n","887          888         1       1                         Graham, Miss. Margaret Edith  female  19.0      0      0            112053   \n","888          889         0       3             Johnston, Miss. Catherine Helen \"Carrie\"  female   NaN      1      2        W./C. 6607   \n","889          890         1       1                                Behr, Mr. Karl Howell    male  26.0      0      0            111369   \n","890          891         0       3                                  Dooley, Mr. Patrick    male  32.0      0      0            370376   \n","\n","        Fare Cabin Embarked  \n","0     7.2500   NaN        S  \n","1    71.2833   C85        C  \n","2     7.9250   NaN        S  \n","3    53.1000  C123        S  \n","4     8.0500   NaN        S  \n","..       ...   ...      ...  \n","886  13.0000   NaN        S  \n","887  30.0000   B42        S  \n","888  23.4500   NaN        S  \n","889  30.0000  C148        C  \n","890   7.7500   NaN        Q  \n","\n","[891 rows x 12 columns]"]},"execution_count":129,"metadata":{},"output_type":"execute_result"}],"source":["df = pd.read_csv(data_path + \"train.csv\")\n","df"]},{"cell_type":"markdown","metadata":{},"source":["### Handling na values\n","For linear regression to work we need numerical values, n/a values are not numerical so we should check if our data set contain them."]},{"cell_type":"code","execution_count":130,"metadata":{},"outputs":[{"data":{"text/plain":["PassengerId      0\n","Survived         0\n","Pclass           0\n","Name             0\n","Sex              0\n","Age            177\n","SibSp            0\n","Parch            0\n","Ticket           0\n","Fare             0\n","Cabin          687\n","Embarked         2\n","dtype: int64"]},"execution_count":130,"metadata":{},"output_type":"execute_result"}],"source":["df.isna().sum()"]},{"cell_type":"markdown","metadata":{},"source":["We should avoid removing columns or rows. Even the absence of data can sometimes indicate a pattern.\n","\n","There are many ways to substitute na_values, the easiest of which is to replace na values with the mode value (the most commonly occuring value). This is a good starting point as usually the method of substituion doesn't have a large impact on our results so the mode is good to get an MVP up and running we can iterate on."]},{"cell_type":"code","execution_count":131,"metadata":{},"outputs":[{"data":{"text/plain":["PassengerId                      1\n","Survived                       0.0\n","Pclass                         3.0\n","Name           Abbing, Mr. Anthony\n","Sex                           male\n","Age                           24.0\n","SibSp                          0.0\n","Parch                          0.0\n","Ticket                        1601\n","Fare                          8.05\n","Cabin                      B96 B98\n","Embarked                         S\n","Name: 0, dtype: object"]},"execution_count":131,"metadata":{},"output_type":"execute_result"}],"source":["modes = df.mode().iloc[0]\n","modes"]},{"cell_type":"code","execution_count":132,"metadata":{},"outputs":[{"data":{"text/plain":["PassengerId    0\n","Survived       0\n","Pclass         0\n","Name           0\n","Sex            0\n","Age            0\n","SibSp          0\n","Parch          0\n","Ticket         0\n","Fare           0\n","Cabin          0\n","Embarked       0\n","dtype: int64"]},"execution_count":132,"metadata":{},"output_type":"execute_result"}],"source":["df.fillna(modes, inplace=True)\n","df.isna().sum()"]},{"cell_type":"code","execution_count":133,"metadata":{},"outputs":[],"source":["def substitue_na_with_modes(df: pd.DataFrame) -> pd.DataFrame:\n","    modes = df.mode().iloc[0]\n","    return df.fillna(modes)"]},{"cell_type":"markdown","metadata":{},"source":["### Converting Category Data to Binary Categorical Values\n"]},{"cell_type":"markdown","metadata":{},"source":["We can get view our non-numeric or numberic data using the describe function.\n"]},{"cell_type":"code","execution_count":134,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Name</th>\n","      <th>Sex</th>\n","      <th>Ticket</th>\n","      <th>Cabin</th>\n","      <th>Embarked</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>891</td>\n","      <td>891</td>\n","      <td>891</td>\n","      <td>891</td>\n","      <td>891</td>\n","    </tr>\n","    <tr>\n","      <th>unique</th>\n","      <td>891</td>\n","      <td>2</td>\n","      <td>681</td>\n","      <td>147</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>top</th>\n","      <td>Braund, Mr. Owen Harris</td>\n","      <td>male</td>\n","      <td>347082</td>\n","      <td>B96 B98</td>\n","      <td>S</td>\n","    </tr>\n","    <tr>\n","      <th>freq</th>\n","      <td>1</td>\n","      <td>577</td>\n","      <td>7</td>\n","      <td>691</td>\n","      <td>646</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                           Name   Sex  Ticket    Cabin Embarked\n","count                       891   891     891      891      891\n","unique                      891     2     681      147        3\n","top     Braund, Mr. Owen Harris  male  347082  B96 B98        S\n","freq                          1   577       7      691      646"]},"execution_count":134,"metadata":{},"output_type":"execute_result"}],"source":["df.describe(include=[object])"]},{"cell_type":"markdown","metadata":{},"source":["Sex and Embarked only have 2, and 3 unique values respectively. It's safe to say these are categorical values.\n","\n","We should also check if any of our numbers are categorical"]},{"cell_type":"code","execution_count":135,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>PassengerId</th>\n","      <th>Survived</th>\n","      <th>Pclass</th>\n","      <th>Age</th>\n","      <th>SibSp</th>\n","      <th>Parch</th>\n","      <th>Fare</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>891.000000</td>\n","      <td>891.000000</td>\n","      <td>891.000000</td>\n","      <td>891.000000</td>\n","      <td>891.000000</td>\n","      <td>891.000000</td>\n","      <td>891.000000</td>\n","    </tr>\n","    <tr>\n","      <th>mean</th>\n","      <td>446.000000</td>\n","      <td>0.383838</td>\n","      <td>2.308642</td>\n","      <td>28.566970</td>\n","      <td>0.523008</td>\n","      <td>0.381594</td>\n","      <td>32.204208</td>\n","    </tr>\n","    <tr>\n","      <th>std</th>\n","      <td>257.353842</td>\n","      <td>0.486592</td>\n","      <td>0.836071</td>\n","      <td>13.199572</td>\n","      <td>1.102743</td>\n","      <td>0.806057</td>\n","      <td>49.693429</td>\n","    </tr>\n","    <tr>\n","      <th>min</th>\n","      <td>1.000000</td>\n","      <td>0.000000</td>\n","      <td>1.000000</td>\n","      <td>0.420000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>25%</th>\n","      <td>223.500000</td>\n","      <td>0.000000</td>\n","      <td>2.000000</td>\n","      <td>22.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>7.910400</td>\n","    </tr>\n","    <tr>\n","      <th>50%</th>\n","      <td>446.000000</td>\n","      <td>0.000000</td>\n","      <td>3.000000</td>\n","      <td>24.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>14.454200</td>\n","    </tr>\n","    <tr>\n","      <th>75%</th>\n","      <td>668.500000</td>\n","      <td>1.000000</td>\n","      <td>3.000000</td>\n","      <td>35.000000</td>\n","      <td>1.000000</td>\n","      <td>0.000000</td>\n","      <td>31.000000</td>\n","    </tr>\n","    <tr>\n","      <th>max</th>\n","      <td>891.000000</td>\n","      <td>1.000000</td>\n","      <td>3.000000</td>\n","      <td>80.000000</td>\n","      <td>8.000000</td>\n","      <td>6.000000</td>\n","      <td>512.329200</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["       PassengerId    Survived      Pclass         Age       SibSp       Parch        Fare\n","count   891.000000  891.000000  891.000000  891.000000  891.000000  891.000000  891.000000\n","mean    446.000000    0.383838    2.308642   28.566970    0.523008    0.381594   32.204208\n","std     257.353842    0.486592    0.836071   13.199572    1.102743    0.806057   49.693429\n","min       1.000000    0.000000    1.000000    0.420000    0.000000    0.000000    0.000000\n","25%     223.500000    0.000000    2.000000   22.000000    0.000000    0.000000    7.910400\n","50%     446.000000    0.000000    3.000000   24.000000    0.000000    0.000000   14.454200\n","75%     668.500000    1.000000    3.000000   35.000000    1.000000    0.000000   31.000000\n","max     891.000000    1.000000    3.000000   80.000000    8.000000    6.000000  512.329200"]},"execution_count":135,"metadata":{},"output_type":"execute_result"}],"source":["df.describe(include=[np.number])"]},{"cell_type":"markdown","metadata":{},"source":["We can see from its quarile values that PClass is likely also categorical despite being numeric as its only values are 1, 2 or 3. We can confirm this by looking at the [data dictionary](https://www.kaggle.com/competitions/titanic/data) for the kaggle competition and by via pandas.\n"]},{"cell_type":"code","execution_count":136,"metadata":{},"outputs":[{"data":{"text/plain":["array([3, 1, 2])"]},"execution_count":136,"metadata":{},"output_type":"execute_result"}],"source":["df.Pclass.unique()"]},{"cell_type":"markdown","metadata":{},"source":["\n","Sex, the Passenger class and Embarking city are not measurable attributes so we should convert them to Boolean numbers that can be used as co-efficients. In the previous notebook we did this manually however this pandas can do this for us using `Dataframe.get_dummies()`"]},{"cell_type":"code","execution_count":137,"metadata":{"execution":{"iopub.execute_input":"2023-12-15T18:36:11.664903Z","iopub.status.busy":"2023-12-15T18:36:11.664551Z","iopub.status.idle":"2023-12-15T18:36:11.691977Z","shell.execute_reply":"2023-12-15T18:36:11.690666Z","shell.execute_reply.started":"2023-12-15T18:36:11.664869Z"},"trusted":true},"outputs":[{"data":{"text/plain":["Index(['PassengerId', 'Survived', 'Name', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Sex_female', 'Sex_male', 'Embarked_C',\n","       'Embarked_Q', 'Embarked_S', 'Pclass_1', 'Pclass_2', 'Pclass_3'],\n","      dtype='object')"]},"execution_count":137,"metadata":{},"output_type":"execute_result"}],"source":["categorical_feature_names = ['Sex', 'Embarked', 'Pclass']\n","df = pd.get_dummies(df, columns=categorical_feature_names, dtype=int)\n","df.columns"]},{"cell_type":"code","execution_count":138,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Sex_male</th>\n","      <th>Sex_female</th>\n","      <th>Pclass_1</th>\n","      <th>Pclass_2</th>\n","      <th>Pclass_3</th>\n","      <th>Embarked_C</th>\n","      <th>Embarked_Q</th>\n","      <th>Embarked_S</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Sex_male  Sex_female  Pclass_1  Pclass_2  Pclass_3  Embarked_C  Embarked_Q  Embarked_S\n","0         1           0         0         0         1           0           0           1\n","1         0           1         1         0         0           1           0           0\n","2         0           1         0         0         1           0           0           1\n","3         0           1         1         0         0           0           0           1\n","4         1           0         0         0         1           0           0           1"]},"execution_count":138,"metadata":{},"output_type":"execute_result"}],"source":["dummy_column_names = ['Sex_male', 'Sex_female', 'Pclass_1', 'Pclass_2', 'Pclass_3', 'Embarked_C', 'Embarked_Q',\n","       'Embarked_S']\n","df[dummy_column_names].head()"]},{"cell_type":"code","execution_count":139,"metadata":{},"outputs":[],"source":["def convert_categories_to_binary_values(df: pd.DataFrame) -> pd.DataFrame:\n","    categorical_feature_names = ['Sex', 'Embarked', 'Pclass']\n","    return pd.get_dummies(df, columns=categorical_feature_names, dtype=int)"]},{"cell_type":"markdown","metadata":{},"source":["### Handling long-tail numerical data"]},{"cell_type":"code","execution_count":140,"metadata":{},"outputs":[{"data":{"text/plain":["<Axes: >"]},"execution_count":140,"metadata":{},"output_type":"execute_result"},{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["import matplotlib\n","df.Fare.hist()"]},{"cell_type":"markdown","metadata":{},"source":["The `Fare` column has lots of small values with the occasional very large value. Uniform normalization using the max value isn't ideal when we're dealing with lots of small values with occasional very large values as the variation between the lower numbers will be lost. To normalize the values we can use a log function (log10 here) to bring the numbers down to reasonable ranges. We must use `log10(x+1)` to avoid 0 values as `log10(0)` would give us infinity."]},{"cell_type":"code","execution_count":141,"metadata":{"execution":{"iopub.execute_input":"2023-12-15T18:36:11.771533Z","iopub.status.busy":"2023-12-15T18:36:11.771187Z","iopub.status.idle":"2023-12-15T18:36:11.795392Z","shell.execute_reply":"2023-12-15T18:36:11.794544Z","shell.execute_reply.started":"2023-12-15T18:36:11.771504Z"},"trusted":true},"outputs":[{"data":{"text/plain":["<Axes: >"]},"execution_count":141,"metadata":{},"output_type":"execute_result"},{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["import math\n","df['LogFare'] = np.log(df['Fare'] + 1)\n","df.LogFare.hist()"]},{"cell_type":"markdown","metadata":{},"source":["## Linear Regression"]},{"cell_type":"markdown","metadata":{},"source":["### Pytorch Tensors\n","For our gradient descent we'll be using Pytorch rather than numpy for this workbook as it will do a lot of the heavy lifting for us. Alongside Tensorflow pytorch is the most commonly used framework for machine learning."]},{"cell_type":"markdown","metadata":{},"source":["We'll start by creating Tensors for our target values (known survival status) and features (our numerical data)."]},{"cell_type":"code","execution_count":142,"metadata":{},"outputs":[{"data":{"text/plain":["tensor([0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1,\n","        1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0,\n","        1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,\n","        0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,\n","        1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,\n","        0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,\n","        0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,\n","        0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0,\n","        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0,\n","        1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n","        1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0,\n","        0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n","        1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1,\n","        0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n","        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,\n","        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,\n","        0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n","        0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,\n","        0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,\n","        1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0])"]},"execution_count":142,"metadata":{},"output_type":"execute_result"}],"source":["from torch import tensor\n","target_tensor = tensor(df.Survived)\n","target_tensor"]},{"cell_type":"code","execution_count":143,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Age</th>\n","      <th>SibSp</th>\n","      <th>Parch</th>\n","      <th>LogFare</th>\n","      <th>Sex_male</th>\n","      <th>Sex_female</th>\n","      <th>Pclass_1</th>\n","      <th>Pclass_2</th>\n","      <th>Pclass_3</th>\n","      <th>Embarked_C</th>\n","      <th>Embarked_Q</th>\n","      <th>Embarked_S</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>22.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>2.110213</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>38.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>4.280593</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>26.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2.188856</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>35.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>3.990834</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>35.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2.202765</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>886</th>\n","      <td>27.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2.639057</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>887</th>\n","      <td>19.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>3.433987</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>888</th>\n","      <td>24.0</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>3.196630</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>889</th>\n","      <td>26.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>3.433987</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>890</th>\n","      <td>32.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2.169054</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>891 rows Ã— 12 columns</p>\n","</div>"],"text/plain":["      Age  SibSp  Parch   LogFare  Sex_male  Sex_female  Pclass_1  Pclass_2  Pclass_3  Embarked_C  Embarked_Q  Embarked_S\n","0    22.0      1      0  2.110213         1           0         0         0         1           0           0           1\n","1    38.0      1      0  4.280593         0           1         1         0         0           1           0           0\n","2    26.0      0      0  2.188856         0           1         0         0         1           0           0           1\n","3    35.0      1      0  3.990834         0           1         1         0         0           0           0           1\n","4    35.0      0      0  2.202765         1           0         0         0         1           0           0           1\n","..    ...    ...    ...       ...       ...         ...       ...       ...       ...         ...         ...         ...\n","886  27.0      0      0  2.639057         1           0         0         1         0           0           0           1\n","887  19.0      0      0  3.433987         0           1         1         0         0           0           0           1\n","888  24.0      1      2  3.196630         0           1         0         0         1           0           0           1\n","889  26.0      0      0  3.433987         1           0         1         0         0           1           0           0\n","890  32.0      0      0  2.169054         1           0         0         0         1           0           1           0\n","\n","[891 rows x 12 columns]"]},"execution_count":143,"metadata":{},"output_type":"execute_result"}],"source":["feature_names = ['Age', 'SibSp', 'Parch', 'LogFare'] + dummy_column_names\n","feature_df = df[feature_names]\n","feature_df"]},{"cell_type":"code","execution_count":144,"metadata":{},"outputs":[{"data":{"text/plain":["tensor([[22.0000,  1.0000,  0.0000,  2.1102,  1.0000,  0.0000,  0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000],\n","        [38.0000,  1.0000,  0.0000,  4.2806,  0.0000,  1.0000,  1.0000,  0.0000,  0.0000,  1.0000,  0.0000,  0.0000],\n","        [26.0000,  0.0000,  0.0000,  2.1889,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000],\n","        [35.0000,  1.0000,  0.0000,  3.9908,  0.0000,  1.0000,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n","        [35.0000,  0.0000,  0.0000,  2.2028,  1.0000,  0.0000,  0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000],\n","        [24.0000,  0.0000,  0.0000,  2.2469,  1.0000,  0.0000,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000,  0.0000],\n","        [54.0000,  0.0000,  0.0000,  3.9677,  1.0000,  0.0000,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n","        ...,\n","        [25.0000,  0.0000,  0.0000,  2.0857,  1.0000,  0.0000,  0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000],\n","        [39.0000,  0.0000,  5.0000,  3.4054,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000,  0.0000],\n","        [27.0000,  0.0000,  0.0000,  2.6391,  1.0000,  0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n","        [19.0000,  0.0000,  0.0000,  3.4340,  0.0000,  1.0000,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n","        [24.0000,  1.0000,  2.0000,  3.1966,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000],\n","        [26.0000,  0.0000,  0.0000,  3.4340,  1.0000,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,  0.0000,  0.0000],\n","        [32.0000,  0.0000,  0.0000,  2.1691,  1.0000,  0.0000,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000,  0.0000]])"]},"execution_count":144,"metadata":{},"output_type":"execute_result"}],"source":["features = feature_df.values\n","feature_tensor = tensor(features, dtype=torch.float)\n","feature_tensor"]},{"cell_type":"markdown","metadata":{},"source":["### Normalization\n","Once all our features are numerical we need to ensure they're somewhat uniform. For Linear regression and many other ML methods having some features be much larger than others will disrupt the process. Rather than do this manually we can have Pytorch do this for us."]},{"cell_type":"code","execution_count":145,"metadata":{},"outputs":[{"data":{"text/plain":["tensor([80.0000,  8.0000,  6.0000,  6.2409,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000])"]},"execution_count":145,"metadata":{},"output_type":"execute_result"}],"source":["max_values, max_indices = feature_tensor.max(dim=0)\n","max_values"]},{"cell_type":"code","execution_count":146,"metadata":{},"outputs":[{"data":{"text/plain":["tensor([[0.2750, 0.1250, 0.0000, 0.3381, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000],\n","        [0.4750, 0.1250, 0.0000, 0.6859, 0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000],\n","        [0.3250, 0.0000, 0.0000, 0.3507, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000],\n","        [0.4375, 0.1250, 0.0000, 0.6395, 0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000],\n","        [0.4375, 0.0000, 0.0000, 0.3530, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000],\n","        [0.3000, 0.0000, 0.0000, 0.3600, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000],\n","        [0.6750, 0.0000, 0.0000, 0.6358, 1.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000],\n","        ...,\n","        [0.3125, 0.0000, 0.0000, 0.3342, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000],\n","        [0.4875, 0.0000, 0.8333, 0.5456, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000],\n","        [0.3375, 0.0000, 0.0000, 0.4229, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000],\n","        [0.2375, 0.0000, 0.0000, 0.5502, 0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000],\n","        [0.3000, 0.1250, 0.3333, 0.5122, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000],\n","        [0.3250, 0.0000, 0.0000, 0.5502, 1.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000],\n","        [0.4000, 0.0000, 0.0000, 0.3476, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000]])"]},"execution_count":146,"metadata":{},"output_type":"execute_result"}],"source":["feature_tensor = feature_tensor / max_values\n","feature_tensor"]},{"cell_type":"markdown","metadata":{},"source":["#### Broadcasting\n","`feature_tensor / max_values` is an example of [broadcasting](https://pytorch.org/docs/stable/notes/broadcasting.html). \n","`max_values` is a one dimensional vector with shape (12). `feature_tensor` is a 2 dimensional matrix with shape (892,12). Because `max_values` is the same size as one of `feature_tensor`'s it will be applied to all 891 rows of `feature_tensor`\n","\n","Broadcasting is useful for large datasets. The calculations are optimized and run on a GPU when available."]},{"cell_type":"markdown","metadata":{},"source":["### Prepare initial linear co-efficient values\n","For linear regression we'd like a one dimensional vector of coefficients equal to our number of rows. Unlike in previous examples we don't need a constant as our dummy variables effectively act as a constant."]},{"cell_type":"code","execution_count":147,"metadata":{"execution":{"iopub.execute_input":"2023-12-15T18:36:11.820322Z","iopub.status.busy":"2023-12-15T18:36:11.819991Z","iopub.status.idle":"2023-12-15T18:36:11.833908Z","shell.execute_reply":"2023-12-15T18:36:11.832743Z","shell.execute_reply.started":"2023-12-15T18:36:11.820295Z"},"trusted":true},"outputs":[{"data":{"text/plain":["tensor([-0.4629,  0.1386,  0.2409, -0.2262, -0.2632, -0.3147,  0.4876,  0.3136,  0.2799, -0.4392,  0.2103,  0.3625])"]},"execution_count":147,"metadata":{},"output_type":"execute_result"}],"source":["torch.manual_seed(442)\n","feature_count = feature_tensor.shape[1]\n","coefficients = torch.rand(feature_count) - 0.5\n","coefficients"]},{"cell_type":"markdown","metadata":{},"source":["Generally we don't want to set a manual seed so we can be aware of how stable our data is or isn't. However for the sake of this lesson I'd like to check I'm getting consistent results with the lesson plan."]},{"cell_type":"markdown","metadata":{},"source":["### Create Predictions\n","We calculate the linear function of our parameters by multiplied them against our random Coefficients then summing each row of weighted values up to create a prediction for each passenger\n","Pytorch's broadcasting can once again be used here to simplify things considerably. We'll print it out to check there aren't any weighted values that are significantly oversized."]},{"cell_type":"code","execution_count":148,"metadata":{"execution":{"iopub.execute_input":"2023-12-15T18:36:11.836343Z","iopub.status.busy":"2023-12-15T18:36:11.835997Z","iopub.status.idle":"2023-12-15T18:36:11.858845Z","shell.execute_reply":"2023-12-15T18:36:11.857637Z","shell.execute_reply.started":"2023-12-15T18:36:11.836315Z"},"trusted":true},"outputs":[{"data":{"text/plain":["tensor([[-0.1273,  0.0173,  0.0000, -0.0765, -0.2632, -0.0000,  0.0000,  0.0000,  0.2799, -0.0000,  0.0000,  0.3625],\n","        [-0.2199,  0.0173,  0.0000, -0.1551, -0.0000, -0.3147,  0.4876,  0.0000,  0.0000, -0.4392,  0.0000,  0.0000],\n","        [-0.1504,  0.0000,  0.0000, -0.0793, -0.0000, -0.3147,  0.0000,  0.0000,  0.2799, -0.0000,  0.0000,  0.3625],\n","        [-0.2025,  0.0173,  0.0000, -0.1446, -0.0000, -0.3147,  0.4876,  0.0000,  0.0000, -0.0000,  0.0000,  0.3625]])"]},"execution_count":148,"metadata":{},"output_type":"execute_result"}],"source":["weighted_values = feature_tensor * coefficients\n","weighted_values[:4]"]},{"cell_type":"code","execution_count":149,"metadata":{},"outputs":[{"data":{"text/plain":["tensor([ 0.1927, -0.6239,  0.0979,  0.2056,  0.0968,  0.0066,  0.1306,  0.3476,  0.1613, -0.6285])"]},"execution_count":149,"metadata":{},"output_type":"execute_result"}],"source":["predictions = weighted_values.sum(dim=1)\n","predictions[:10]"]},{"cell_type":"markdown","metadata":{},"source":["### Calculate loss\n","Our loss here is the average difference between our prediction value and whether the passegner survived or not (1 or 0)."]},{"cell_type":"code","execution_count":150,"metadata":{},"outputs":[{"data":{"text/plain":["tensor(0.5382)"]},"execution_count":150,"metadata":{},"output_type":"execute_result"}],"source":["loss = torch.abs(predictions - target_tensor).mean()\n","loss"]},{"cell_type":"code","execution_count":151,"metadata":{},"outputs":[],"source":["def create_predictions(features: torch.Tensor, coefficients: torch.Tensor) -> torch.Tensor:\n","    return (coefficients * features).sum(dim=1)"]},{"cell_type":"code","execution_count":152,"metadata":{},"outputs":[],"source":["def calculate_loss(features: torch.Tensor, coefficients: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n","    predictions = create_predictions(features, coefficients=coefficients)\n","    return torch.abs(predictions - targets).mean()"]},{"cell_type":"markdown","metadata":{},"source":["### Doing a single Gradient Descent step\n","Now we want to optimize our loss with gradient descent. This too will be significantly easier using Pytorch as it will calculate the gradient for us.\n","\n","We must tell pytorch to store the results of each coefficient calculation so we can get the gradients from it later."]},{"cell_type":"code","execution_count":153,"metadata":{"execution":{"iopub.execute_input":"2023-12-15T18:36:11.860841Z","iopub.status.busy":"2023-12-15T18:36:11.860409Z","iopub.status.idle":"2023-12-15T18:36:22.655866Z","shell.execute_reply":"2023-12-15T18:36:22.654577Z","shell.execute_reply.started":"2023-12-15T18:36:11.860799Z"},"trusted":true},"outputs":[{"data":{"text/plain":["tensor([-0.4629,  0.1386,  0.2409, -0.2262, -0.2632, -0.3147,  0.4876,  0.3136,  0.2799, -0.4392,  0.2103,  0.3625], requires_grad=True)"]},"execution_count":153,"metadata":{},"output_type":"execute_result"}],"source":["coefficients.requires_grad_()"]},{"cell_type":"code","execution_count":154,"metadata":{},"outputs":[{"data":{"text/plain":["tensor(0.5382, grad_fn=<MeanBackward0>)"]},"execution_count":154,"metadata":{},"output_type":"execute_result"}],"source":["loss = calculate_loss(feature_tensor, coefficients=coefficients, targets=target_tensor)\n","loss"]},{"cell_type":"markdown","metadata":{},"source":["The loss is in a tensor where can ask Pytorch to calculate the gradient by calling `backward()`"]},{"cell_type":"code","execution_count":155,"metadata":{},"outputs":[{"data":{"text/plain":["tensor([-0.0106,  0.0129, -0.0041, -0.0484,  0.2099, -0.2132, -0.1212, -0.0247,  0.1425, -0.1886, -0.0191,  0.2043])"]},"execution_count":155,"metadata":{},"output_type":"execute_result"}],"source":["loss.backward()\n","coefficients.grad"]},{"cell_type":"markdown","metadata":{},"source":["Here we perform one gradient descent step\n"]},{"cell_type":"code","execution_count":156,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor(0.5197)\n"]}],"source":["loss = calculate_loss(feature_tensor, coefficients=coefficients, targets=target_tensor)\n","loss.backward\n","with torch.no_grad():\n","    assert coefficients.grad is not None\n","    coefficients.sub_(coefficients.grad * 0.1)\n","    coefficients.grad.zero_()\n","    print(calculate_loss(feature_tensor, coefficients=coefficients, targets=target_tensor))"]},{"cell_type":"markdown","metadata":{},"source":["A few points:\n","1. `torch.no_grad()` is required to ensure the parameter update step is peformed without tracking gradients. We want to track gradients for the forward and backward steps but not when directly modifying the parameters\n","2. `coefficients.sub_(coefficients.grad * 0.1)` reduces the coefficients by their gradient to the loss. More significant features will be reduced more. \n","3. Both `sub_` and `zero_` operations are done in place for memory efficiency and to preserve the tensors memory graph (this is also ensured by `torch.no_grad()` although it's good practice when working with tensors).\n","4. `coefficients.grad.zero_()` sets our gradients to zero. This is necessary as if we were to do another backpass the new gradients would be added to the old ones."]},{"cell_type":"markdown","metadata":{},"source":["### Creating a validation set\n"]},{"cell_type":"markdown","metadata":{},"source":["Before we begin training we need a validation set to compare our training data against."]},{"cell_type":"markdown","metadata":{},"source":["I've deviated from the [fast.ai kaggle workbook](https://www.kaggle.com/code/jhoward/linear-model-and-neural-net-from-scratch) as they split their validation set using the fastai library to keep things consistent for their next chapter. I'm interested in primarily learning Pytorch so I'm going to split the dataset without the fastai library. However so I can check if my results match fast.ai's I'm going to include their splitter here it will be used if `use_fastai_splitter` is set to `True` so I can check my results are consistent with the fast.ai tutorials."]},{"cell_type":"code","execution_count":157,"metadata":{},"outputs":[],"source":["from random import Random\n","from numpy import int64\n","from fastai.data.transforms import RandomSplitter\n","from typing import Tuple, List, cast\n","from fastcore.foundation import L\n","from torch import Tensor\n","\n","def split_data_with_fastai(df: pd.DataFrame) -> Tuple[Tensor,Tensor]:\n","    train_indices, validation_indices = RandomSplitter(seed=42)(df)\n","    return torch.tensor(train_indices, dtype=torch.int64), torch.tensor(validation_indices, dtype=torch.int64)"]},{"cell_type":"markdown","metadata":{},"source":["First we'll split our data"]},{"cell_type":"code","execution_count":158,"metadata":{},"outputs":[{"data":{"text/plain":["(713, 178)"]},"execution_count":158,"metadata":{},"output_type":"execute_result"}],"source":["use_fastai_splitter = True\n","total_passengers = feature_tensor.size(0)\n","training_set_size = int(total_passengers * 0.8)\n","\n","if use_fastai_splitter:\n","    train_indices, validation_indices = split_data_with_fastai(df)\n","else:\n","    randomized_indices = torch.randperm(total_passengers)\n","    train_indices = randomized_indices[:training_set_size]\n","    validation_indices = randomized_indices[training_set_size:]\n","\n","training_features = feature_tensor[train_indices]\n","validation_features = feature_tensor[validation_indices]\n","training_targets = target_tensor[train_indices]\n","validation_targets = target_tensor[validation_indices]\n","len(training_features), len(validation_features)"]},{"cell_type":"markdown","metadata":{},"source":["This note book doesn't use Pytorch's `Dataset`s. We'd likely use these in a real project although for this example we're keeping things a bit barer than normal so we can see the process."]},{"cell_type":"markdown","metadata":{},"source":["We'll add what we've done so far in to functions to make things easier to read and re-usable."]},{"cell_type":"code","execution_count":159,"metadata":{},"outputs":[],"source":["def update_coefficients(coefficients, learning_rate):\n","    coefficients.sub_(coefficients.grad * learning_rate)\n","    coefficients.grad.zero_()"]},{"cell_type":"code","execution_count":160,"metadata":{},"outputs":[],"source":["def one_epoch(coefficients, learning_rate):\n","    loss = calculate_loss(training_features, coefficients, training_targets)\n","    loss.backward()\n","    with torch.no_grad():\n","        update_coefficients(coefficients, learning_rate=learning_rate)\n","        \n","    print(f\"{loss:.3f}\", end=\"; \")"]},{"cell_type":"code","execution_count":161,"metadata":{},"outputs":[],"source":["def generate_coefficients(features: torch.Tensor) -> torch.Tensor:\n","    coefficient_count = features.shape[1]\n","    coefficients = torch.rand(coefficient_count) - 0.5\n","    coefficients.requires_grad_()\n","    return coefficients"]},{"cell_type":"markdown","metadata":{},"source":["Now to train the model"]},{"cell_type":"code","execution_count":162,"metadata":{},"outputs":[],"source":["def train_model(epoch_count=30, learning_rate=0.1):\n","    coefficients = generate_coefficients(training_features)\n","    for i in range(epoch_count):\n","        one_epoch(coefficients, learning_rate=learning_rate)\n","    return coefficients"]},{"cell_type":"code","execution_count":163,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["0.550; 0.494; 0.446; 0.400; 0.385; 0.392; 0.348; 0.351; 0.365; 0.327; 0.345; 0.310; 0.326; 0.292; 0.308; 0.280; 0.323; 0.269; "]},{"data":{"text/plain":["tensor([ 0.0114, -0.1277, -0.0490,  0.2052,  0.0136,  0.7174,  0.0278, -0.1602, -0.1642,  0.1394,  0.3767,  0.2155], requires_grad=True)"]},"execution_count":163,"metadata":{},"output_type":"execute_result"}],"source":["coefficients = train_model(epoch_count=18, learning_rate=0.2)\n","coefficients"]},{"cell_type":"markdown","metadata":{},"source":["We can see below that our models has optimized our weights to reduce our loss. From this we can see that the model believes"]},{"cell_type":"code","execution_count":164,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Feature</th>\n","      <th>Coefficient</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Age</td>\n","      <td>0.011448</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>SibSp</td>\n","      <td>-0.127748</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Parch</td>\n","      <td>-0.049038</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>LogFare</td>\n","      <td>0.205193</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Sex_male</td>\n","      <td>0.013623</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>Sex_female</td>\n","      <td>0.717442</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>Pclass_1</td>\n","      <td>0.027782</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>Pclass_2</td>\n","      <td>-0.160171</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>Pclass_3</td>\n","      <td>-0.164208</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>Embarked_C</td>\n","      <td>0.139427</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>Embarked_Q</td>\n","      <td>0.376680</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>Embarked_S</td>\n","      <td>0.215454</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["       Feature  Coefficient\n","0          Age     0.011448\n","1        SibSp    -0.127748\n","2        Parch    -0.049038\n","3      LogFare     0.205193\n","4     Sex_male     0.013623\n","5   Sex_female     0.717442\n","6     Pclass_1     0.027782\n","7     Pclass_2    -0.160171\n","8     Pclass_3    -0.164208\n","9   Embarked_C     0.139427\n","10  Embarked_Q     0.376680\n","11  Embarked_S     0.215454"]},"metadata":{},"output_type":"display_data"}],"source":["def show_coeffs(): \n","    coeff_array = [coeff.item() for coeff in coefficients]\n","    coeff_df = pd.DataFrame({'Feature': feature_names, 'Coefficient': coeff_array})\n","    display(coeff_df)\n","show_coeffs()"]},{"cell_type":"markdown","metadata":{},"source":["### Measuring accuracy"]},{"cell_type":"markdown","metadata":{},"source":["To view our accuracy we'll now use our validation set. We'll create predictions using our newly trained coefficients and see how accurate they are."]},{"cell_type":"code","execution_count":165,"metadata":{},"outputs":[{"data":{"text/plain":["tensor([1.0226, 0.3008, 0.0616, 0.2132, 0.1593, 0.1594, 0.7728, 0.8635, 0.0960, 0.7756], grad_fn=<SliceBackward0>)"]},"execution_count":165,"metadata":{},"output_type":"execute_result"}],"source":["predictions = create_predictions(validation_features, coefficients=coefficients)\n","predictions[:10]"]},{"cell_type":"markdown","metadata":{},"source":["If our predictions is >0.5 and the passegner surivied we're correct. If the passenger died we want a prediction < 0.5. 0 = died, 1 = survived. This code merely rounds our predictions to whichever of these values is closest"]},{"cell_type":"code","execution_count":166,"metadata":{},"outputs":[{"data":{"text/plain":["tensor(0.7865)"]},"execution_count":166,"metadata":{},"output_type":"execute_result"}],"source":["results = validation_targets.bool() == (predictions>0.5)\n","results.float().mean()"]},{"cell_type":"markdown","metadata":{},"source":["We're 79% accurate which is pretty good going."]},{"cell_type":"code","execution_count":167,"metadata":{},"outputs":[],"source":["from torch import Tensor\n","\n","\n","def calculate_accuracy(coefficients, features: torch.Tensor) -> Tensor:\n","    predictions = create_predictions(features, coefficients=coefficients)\n","    results = validation_targets.bool() == (predictions>0.5)\n","    return results.float().mean()"]},{"cell_type":"markdown","metadata":{},"source":["### Sigmoid\n","When creating predictions that are between 0 and 1 we can increase our accuracy by using the sigmoid function which moves all our values between 0 and 1 and larger negative or positives values will respectively asymptotically converge towards 0 or 1."]},{"cell_type":"code","execution_count":168,"metadata":{},"outputs":[{"data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAnYAAAHTCAYAAACqbVU5AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAABFRklEQVR4nO3dd3hUZcLG4WfSJgkpENIghN5bQgJEbKCLoiKfiAWxgNhdZFV0FVRE1wJrWVFAcNG1I6goFhBFECtIDU1Ch4SQCmmkTTJzvj+C0UgLkMyZzPzu65ork5NzmGccMzycd97zWgzDMAQAAIAGz8vsAAAAAKgbFDsAAAA3QbEDAABwExQ7AAAAN0GxAwAAcBMUOwAAADdBsQMAAHATFDsAbs8wDBUWForLdgJwdxQ7AG6vqKhIoaGhKioqMjsKANQrih0AAICboNgBAAC4CYodAACAm6DYAQAAuAmKHQAAgJug2AEAALgJih0AAICboNgBAAC4CYodAACAm6DYAQAAuAmKHQAAgJug2AEAALgJih0AAICboNgBAAC4CYodAACAm6DYAXCqH374QUOGDFHz5s1lsVi0YMGCkx6zfPlyJSQkyGq1qn379nrrrbfqPScANEQUOwBOVVxcrLi4OM2YMaNW++/Zs0eDBw/WBRdcoOTkZN1333267bbb9PXXX9dzUgBoeCyGYRhmhwDgmSwWiz799FMNHTr0uPs8/PDDWrhwoTZv3ly97brrrlN+fr4WL15cq8cpLCxUaGioCgoKFBIScqaxAcBlccYOgEtbsWKFBg4cWGPboEGDtGLFiuMeU15ersLCwho3APAEFDsALi0zM1NRUVE1tkVFRamwsFClpaXHPGby5MkKDQ2tvsXGxjojKgCYzsfsAABQ1yZMmKBx48ZVf19YWEi5A2CqCrtDh8sqVVRWqaLyiur7h8srVWKzq8RWqVKbXaUVdpXY7DXul1XY9d5tSbV6HIodAJcWHR2trKysGtuysrIUEhKigICAYx5jtVpltVqdEQ+AhyirsCuvxKa84grll9iUV1Kh/FKbDh8pZ0W/l7ayCh0uP3pbeaXDKTkpdgBcWr9+/bRo0aIa25YsWaJ+/fqZlAhAQ1dcXqmconIdKrFVlbTiCuWV2JRfUvNrXsnvJc6msoq6KWYBvt4K8vdRsL+Pgq0+CvL3UaCfjwJ8vRXo560AvyNffb0V4OejwCPf1xbFDoBTHT58WDt37qz+fs+ePUpOTlZYWJhatmypCRMmKD09Xe+8844k6a677tL06dP10EMP6ZZbbtGyZcv04YcfauHChWY9BQAuyDAM5ZdUKLuoXNlFZcouLP/jflG5cgqr7ucUlavYZj+tx/D2sqhxgK8aB/qqSaCfGgf6KsTft7qoBVmr7of4+yjI6qNgf98jX6tujaw+8vWu3+kNFDsATrVmzRpdcMEF1d///lm4UaNG6a233lJGRoZSU1Orf96mTRstXLhQ999/v15++WW1aNFCr7/+ugYNGuT07ADMUWl3KKOgTGmHSpSWV6KswprlLefIzWav/Vm1QD9vhTXy+1NJ81OTQN/qr78XtyaBflX3G/kq2Ooji8VSj8/0zHEdOwBuj+vYAa7NMAzlHrYpLa9EaYdKtD+vVKkHq0pcWl6JDuSXye6oXV1pHOiryGCrIoP9FRlsVUTIH/cjg62KDKm638jqnue23PNZAQAAl1JcXqm0vJIjha206uzbkTNwaYdKVVpx4uFRPx8vtWgSoNgmgWoW+ntp+6OwRRy5WX1q/3k0d0SxAwAAdabUZteO7CJtyyzS9qwibcs6rO2ZRcosLDvhcRaL1CzEXy3CAhXbJFCxYQFqGRao2CPfRwZb5eXl2sOgroBiBwAATlmF3aE9ucXVBS7lyNfUQyU63oe8Ggf6VpW1JoFqEVZ19u338ta8sb/Hn22rCxQ7AABwXA6HobS8kqPOwO3OPawK+7EbXHiQnzpGBatjVLA6RVd9bR8ZpNAAXyen9zwUOwAAUC27qEzrU/O1LjVP61PztTm9QCXHuTxIkNVHHaOCqstbp6hgdYwOVngQFwg3C8UOAAAPZat0aGtGYXWJW5eap/15R6/B7OfjpfYRfxS4ztFVBa55qL/LX/7D01DsAADwEJkFZVqfmldd5DalFxy11JXFInWMDFavlo2V0LKJ4ls2VtvwRvKp5wvrom5Q7AAAcFNZhWX6fnuOftyRq7V7D+lAwdEzUxsH+qpXbGP1atlECS2bqGdsqEL8+SxcQ0WxAwDATdgqHVqz75C+356j77flKCWzqMbPvSxSp+iQ6rNxCS0bq014I4ZT3QjFDgCABiz1YIm+356t77fn6JddB2tMdLBYpJ4xoerfMUJntW2qnrGNFeSmKy6gCq8uAAANSImtUr/uPnJWbnuO9uQW1/h5eJBV53cMV/+OETqvQ4TCGvmZlBRmoNgBAODCDMPQjuzD+n5bVZFbtedQjcXufbwsSmzVRP07Rah/xwh1iQ5hhQYPRrEDAMAF7cwu0vx16fo8+YDS82tegiSmcYAGdIrQ+R0jdHa7pgpmsgOOoNgBAOAiDhXb9Hlyuj5Zn66N+wuqt1t9vNSvXVOd3yFC/TtFqC0THnAcFDsAAExUXmnXdynZmr8uXd+lZKvSUbVMl4+XRRd0jtRVCTEa0ClS/r6so4qTo9gBAOBkhmEoOS1fn6xL1xcbDyi/pKL6Zz1bhGpYrxgNiWuupizNhVNEsQMAwEnS80u1YH265q/br905f8xmjQqx6speLTQsIUYdo4JNTIiGjmIHAEA9OlxeqcWbMzV/7X6t3HNQRtVIqwJ8vXVJ92gNS4jR2e3C5c1MVtQBih0AAHXM7jC0YtdBzV+3X4s3Z6q04o+LBvdr21TDEmJ0aY9mXCwYdY7/owAAqCOZBWV6e8VeLVifrow/rcvaJryRrkqI0dBeMWrRJNDEhHB3FDsAAM7Q/rwSzVy+Sx+t2V998eDQAF8NiWumYQkt1Cu2MZcngVNQ7AAAOE17c4v16vKd+mRdevVlSvq2CdPos1vrwi6RsvpwiRI4F8UOAIBTtDO7SNOX7dTnGw7oSJ/Tue3DNfbC9kpq29TccPBoFDsAAGppa0ahpi/bqUWbM6pnt17QKUL3XNhBia2amBsOEMUOAICT2rS/QNOW7dA3v2VVb7u4a5TGXthBPVqEmpgMqIliBwDAcaxLzdO0pTv03bYcSZLFIl3Wo5nuuaC9ujQLMTkdcDSKHQAAf/Hr7oOatmynftqZK0nyskhXxMdozAXt1D6SlSHguih2AACoav3Wn3ce1CvLdmjVnkOSJB8vi4YlxOjvA9qrdXgjkxMCJ0exAwB4NMMwtHxbjl5ZtkPrU/MlSX7eXrqmdwvd1b+dYsO4oDAaDoodAMBjrdh1UM8u2qpN6QWSJKuPl0b0bak7+7dVs9AAk9MBp45iBwDwOEVlFZryVYre/zVVkhTg660bz2qp289vq8hgf5PTAaePYgcA8CjLt2XrkU826cCRtVyvT2qpBy7qqKZBVpOTAWeOYgcA8Aj5JTY99eVWzV+3X5LUMixQU67qobPbhZucDKg7FDsAgNv7ekumHluwWTlF5bJYpNFnt9GDgzoq0I+/BuFe+D8aAOC2Dh4u16TPt+jLjRmSpHYRjfTc1T2V2CrM5GRA/aDYAQDcjmEY+nzDAT3x+RbllVTI28uiO89vq3/8rYP8fb3NjgfUG4odAMCtZBWW6dFPN+vbrVXrunaODtbzV8expis8AsUOAOAWDMPQR2v266mFv6morFK+3haNvbCD7urfTn4+XmbHA5yCYgcAaPD255Vowieb9OOOqrVd41qE6rmr49QpmnVd4VkodgCABsvhMPT+r/s05asUFdvssvp4adxFHXXruW3k481ZOngeih0AoEHak1ush+dv1Ko9hyRJfVo30b+v6qm2EUEmJwPMQ7EDADQodoeh//20Ry98s03llQ4F+nnr4Us666azWsnLy2J2PMBUFDsAQIOxPatI//x4ozak5UuSzmnfVFOG9VRsWKC5wQAXQbEDALi8CrtDs5bv0rRlO2WzOxRs9dFjl3fRtb1jZbFwlg74HcUOAODSNqcX6KGPN+q3jEJJ0t86R+qZK3soOtTf5GSA66HYAQBcUqXdoanf7tDM73fJ7jDUONBXTwzppivim3OWDjgOih0AwOUUlFRozJx1+mln1XXpBvdopif+r5sigq0mJwNcG8UOAOBSducc1m1vr9Hu3GIF+Hrruat7akhcc7NjAQ0CxQ4A4DJ+3JGjMe+vU2FZpZqH+mv2qN7q1pw1XoHaotgBAExnGIbe/mWvnlq4VXaHoYSWjfXaTb0ZegVOEcUOAGCqCrtDj3+2RR+sSpUkDUuI0eRhPWT18TY5GdDwUOwAAKbJK7bp7vfXauXuQ7JYpAmXdtbt57Vl1itwmih2AABTbM8q0m1vr1HqoRIFWX308nXx+luXKLNjAQ0axQ4A4HTLUrL0jw+Sdbi8UrFhAXpjVB91jAo2OxbQ4FHsAABOYxiGZv+4W5O/SpFhSH3bhGnWjYkKa+RndjTALVDsAABOUV5p16OfbtbHa/dLkq7rE6t/XdFdfj5eJicD3AfFDgBQ73IPl+vOd9dq7b48eVmkiZd31c1nt2aSBFDHKHYAgHr124FC3f7OGqXnlyrY30czrk/Q+R0jzI4FuCWKHQCg3vy4I0d3vrtWJTa7WjcN1Ouj+qh9ZJDZsQC3RbEDANSL77fn6PZ31shW6dA57ZtqxvUJahzIJAmgPlHsAAB1bvm2bN3x7lrZKh26qGuUZlyfwCQJwAn4LQMA1KnvUrJ1xztVpe5iSh3gVJyxAwDUmWUpWbrr3XWy2R26pFu0pl3fS77elDrAWSh2AIA6sXRrlu56b60q7IYu7R6tV0ZQ6gBn4zcOAHDGvv3tj1I3uEczSh1gEs7YAQDOyDdbMjVmzrqqUtezmV4eHi8fSh1gCn7zADjdjBkz1Lp1a/n7+yspKUmrVq064f5Tp05Vp06dFBAQoNjYWN1///0qKytzUlqcyOLNmfr7+1Wlbkhcc0odYDJ++wA41bx58zRu3DhNmjRJ69atU1xcnAYNGqTs7Oxj7j9nzhyNHz9ekyZN0tatW/XGG29o3rx5euSRR5ycHH+1eHOG7pmzTpUOQ/8X11wvXRtHqQNMZjEMwzA7BADPkZSUpD59+mj69OmSJIfDodjYWI0dO1bjx48/av977rlHW7du1dKlS6u3PfDAA/r111/1008/1eoxCwsLFRoaqoKCAoWEhNTNE/FwizZlaOwH62V3GBoa31wvXEOpA1wBv4UAnMZms2nt2rUaOHBg9TYvLy8NHDhQK1asOOYxZ599ttauXVs9XLt7924tWrRIl1122XEfp7y8XIWFhTVuqDsLN/5R6ob1itGL1zL8CrgKJk8AcJrc3FzZ7XZFRUXV2B4VFaWUlJRjHnP99dcrNzdX5557rgzDUGVlpe66664TDsVOnjxZTz75ZJ1mR5UvNhzQffOSq0pdQoyevzpO3l4Ws2MBOIJ/YgFwacuXL9ezzz6rV199VevWrdMnn3yihQsX6qmnnjruMRMmTFBBQUH1LS0tzYmJ3ddnyem6d27VmbqrE1tQ6gAXxBk7AE4THh4ub29vZWVl1dielZWl6OjoYx4zceJE3XTTTbrtttskST169FBxcbHuuOMOPfroo/LyOvrfp1arVVarte6fgAf7LDld989LlsOQru3dQlOG9ZQXpQ5wOZyxA+A0fn5+SkxMrDERwuFwaOnSperXr98xjykpKTmqvHl7e0uSmPvlHJ+u319d6q7rE0upA1wYZ+wAONW4ceM0atQo9e7dW3379tXUqVNVXFys0aNHS5JGjhypmJgYTZ48WZI0ZMgQ/ec//1GvXr2UlJSknTt3auLEiRoyZEh1wUP9mb92vx78eIMMQxrRN1bPDO1BqQNcGMUOgFMNHz5cOTk5evzxx5WZman4+HgtXry4ekJFampqjTN0jz32mCwWix577DGlp6crIiJCQ4YM0TPPPGPWU/AYH6/dr38eKXXXJ7XU01d0p9QBLo7r2AFwe1zH7tR9tCZND83fKMOQbjyrpf71f5Q6oCHgjB0AoIYftufo4SOlbmS/Vnry/7rJYqHUAQ0BkycAANV2ZhdpzPvr5DCkqxJaUOqABoZiBwCQJOUV23TLW2tUVF6pPq2b6Nlh3Sl1QANDsQMAyFbp0J3vrVXqoRLFhgVo1o2Jsvow6xhoaCh2AODhDMPQYws2adWeQwqy+uiNUX3UNIgLPAMNEcUOADzcGz/t0Ydr9svLIk27vpc6RgWbHQnAaaLYAYAHW7o1S88s2ipJenRwV13QKdLkRADOBMUOADxUSmah/vHB+iOrSrTULee0NjsSgDNEsQMAD5R7uFy3vrVGxTa7+rVtqn9dwWVNAHdAsQMAD1NWYded765Ven6pWjcN1MwbE+TrzV8HgDvgNxkAPIhhGHrkk01auy9Pwf4+euPmPmoc6Gd2LAB1hGIHAB5k5ve79Mn6dHl7WTTzhkS1iwgyOxKAOkSxAwAPsXhzpp5bvE2S9MT/ddO5HcJNTgSgrlHsAMADbE4v0P3zkiVJo/q10k1ntTI3EIB6QbEDADeXXVim299Zo9IKu87rEK6Jl3c1OxKAekKxAwA3VlZh1+3vrlVGQZnaRTTS9OsT5MMMWMBt8dsNAG7KMAw9+NEGbUjLV+NAX70xqo9CA3zNjgWgHlHsAMBNvbJ0p77cmCGfIzNgW4c3MjsSgHpGsQMAN/TlxgN66dvtkqRnruyufu2ampwIgDNQ7ADAzWxIy9cDH26QJN12bhsN79PS5EQAnIViBwBuJKOgVLe/s0bllQ5d2DlSEy7rYnYkAE5EsQMAN1Fiq9Rtb69RdlG5OkUF6+Xr4uXtZTE7FgAnotgBgBtwOAyNm7dBWw4UqmkjP70+qreC/ZkBC3gaih0AuIH/LNmuxVsy5eftpdduSlRsWKDZkQCYgGIHAA3cp+v3a/p3OyVJk4f1UO/WYSYnAmAWih0ANGBr9+Xp4Y83SZLuHtBOVyW2MDkRADNR7ACggUrPL9Wd766Rze7QxV2j9M+LO5kdCYDJKHYA0ABV2B26Z8465R62qWuzEL00PF5ezIAFPB7FDgAaoJe/3aH1qfkK9vfRazclqpHVx+xIAFwAxQ4AGphfduVqxvKqyRJThvVkBiyAahQ7AGhADhXbdP+8ZBmGdF2fWA3u2czsSABcCMUOABoIwzD00McblFVYrnYRjfT4kK5mRwLgYih2ANBAvLtyn77dmi0/by+9MqKXAv34XB2Amih2ANAAbM0o1NMLt0qSJlzWWd2ah5qcCIArotgBgIsrtdk19oP1slU6dGHnSN18dmuzIwFwURQ7AHBx//ryN+3MPqzIYKuev7qnLBauVwfg2Ch2AODCvtqUoQ9WpcpikV4aHq+mQVazIwFwYRQ7AHBR6fmlenj+RknSXf3b6Zz24SYnAuDqKHYA4IIq7Q7dN3e9CssqFRfbWOMu6mh2JAANAMUOAFzQtGU7tXpvnoKsPpp2XS/5evN2DeDkeKcAABezas8hTVu2Q5L0zJXd1bIpS4YBqB2KHQC4kPwSm+6bu14OQ7oqoYWuiI8xOxKABoRiBwAuwjAMjZ+/SQcKytQmvJGevKKb2ZEANDAUOwBwEXNWpWrxlkz5elv0ynW9FGRlyTAAp4ZiBwAuYHtWkf71xW+SpIcv6aweLVgyDMCpo9gBgMnKKuz6xwfrVV7p0PkdI3TLOW3MjgSggaLYAYDJnl20VSmZRQoP8tOL18TJy4slwwCcHoodAJjomy2ZemfFPknSi9fGKyKYJcMAnD6KHQCYJKOgVA8dWTLsjvPbqn/HCJMTAWjoKHYAYAK7w9D985KVX1KhHjGhevDiTmZHAuAGKHYAYIKZy3dq5e5DCvTz1isjesnPh7djAGeOdxIAcLK1+w7ppW+rlgx76oruahPeyOREANwFxQ4AnKiorEL/+CBZdoehofHNNSyBJcMA1B2KHQA40bOLUpSeX6rYsAA9NbS7LBYubQKg7lDsAMBJftqRqw9WpUqSnr86TsH+viYnAuBuKHYA4ASHyyv18JFLm4zq10pntW1qciIA7ohiBwBOMOWrrdVDsA9d0tnsOADcFMUOAOrZLztz9d7KqiHYfw/rqUZWH5MTAXBXFDsAqEfF5ZV6+JOqIdgbklrq7PbhJicC4M4odgBQj55bnKK0Q6WKaRygCZd1MTsOADdHsQOAerJy90G9vWKfJGnKVT0UxBAsgHpGsQOAelBqs1fPgh3RN1bndYgwOREAT0CxA4B68PzX27TvYImahfozBAvAaSh2AFDHVu89pDd/2SNJmjysh0K4EDEAJ6HYAUAdKrXZ9dDHG2UY0rW9W2hAp0izIwHwIBQ7AKhD/1myTXtyixUVYtWjg7uaHQeAh6HYAXC6GTNmqHXr1vL391dSUpJWrVp1wv3z8/M1ZswYNWvWTFarVR07dtSiRYuclLb21u7L0+s//TEEGxrAECwA52LuPQCnmjdvnsaNG6dZs2YpKSlJU6dO1aBBg7Rt2zZFRh49bGmz2XTRRRcpMjJSH3/8sWJiYrRv3z41btzY+eFPoKzCrn9+vEGGIQ1LiNGFnaPMjgTAA1kMwzDMDgHAcyQlJalPnz6aPn26JMnhcCg2NlZjx47V+PHjj9p/1qxZev7555WSkiJf39M7A1ZYWKjQ0FAVFBQoJCTkjPIfz+Svtuq173crMtiqJff3V2ggZ+sAOB9DsQCcxmazae3atRo4cGD1Ni8vLw0cOFArVqw45jGff/65+vXrpzFjxigqKkrdu3fXs88+K7vdftzHKS8vV2FhYY1bfUpOy9fsH3ZLkp65sgelDoBpKHYAnCY3N1d2u11RUTWHKaOiopSZmXnMY3bv3q2PP/5YdrtdixYt0sSJE/Xiiy/q6aefPu7jTJ48WaGhodW32NjYOn0ef1ZWYdc/P9oghyENjW+ui7oyBAvAPBQ7AC7N4XAoMjJS//3vf5WYmKjhw4fr0Ucf1axZs457zIQJE1RQUFB9S0tLq7d8ryzdoR3ZhxUeZNWkId3q7XEAoDaYPAHAacLDw+Xt7a2srKwa27OyshQdHX3MY5o1ayZfX195e3tXb+vSpYsyMzNls9nk5+d31DFWq1VWq7Vuwx/Dxv35eu3IEOzTQ7urSaOjswCAM3HGDoDT+Pn5KTExUUuXLq3e5nA4tHTpUvXr1++Yx5xzzjnauXOnHA5H9bbt27erWbNmxyx1zlJeadc/P9oou8PQkLjmuqT7sYspADgTxQ6AU40bN06zZ8/W22+/ra1bt+ruu+9WcXGxRo8eLUkaOXKkJkyYUL3/3XffrUOHDunee+/V9u3btXDhQj377LMaM2aMWU9BkjRj2U5tyypS00Z+evL/GIIF4BoYigXgVMOHD1dOTo4ef/xxZWZmKj4+XosXL66eUJGamiovrz/+zRkbG6uvv/5a999/v3r27KmYmBjde++9evjhh816CtqcXqAZy3dJkp4a2l1hDMECcBFcxw6A26vL69jZKh26YsbP2ppRqMt6ROvVGxLrKCUAnDmGYgHgFLy6fKe2ZhSqSaCv/nVFd7PjAEANFDsAqKXfDhRq+rKdkqQnr+iu8KD6n3kLAKeCYgcAtVBhd+ifH29QpcPQoG5RGtKzmdmRAOAoFDsAqIVZy3dpy4FCNQ701VNDu8tisZgdCQCOQrEDgJPYllmkV5btkCQ9MaSbIoP9TU4EAMdGsQOAE6i0O/TgRxtUYTc0sEukrohvbnYkADguih0AnMB/f9ytTekFCvH30TNX9mAIFoBLo9gBwHHsyCrS1CVVQ7CThnRTVAhDsABcG8UOAI7B7jD00PyNstkduqBThIYlxJgdCQBOimIHAMfw3sp9Wp+aryCrj54dxhAsgIaBYgcAf3Egv1TPLU6RJD18SSc1Cw0wOREA1A7FDgD+xDAMPf7ZZhXb7Epo2Vg3JLUyOxIA1BrFDgD+5KvNmfp2a7Z8vS2aclVPeXkxBAug4aDYAcARBSUVmvT5FknS3f3bqWNUsMmJAODUUOwA4Igpi7cqp6hcbSMa6e8XtDc7DgCcMoodAEhaufugPliVJkmaMqyn/H29TU4EAKeOYgfA45VV2PXIJ5skSSP6tlTfNmEmJwKA00OxA+DxZny3U7tzixURbNX4SzubHQcAThvFDoBH25ZZpJnLd0mSnvy/bgoN8DU5EQCcPoodAI/lcBga/8lGVToMDewSpUu7R5sdCQDOCMUOgMd679c/lg17amg3lg0D0OD5mB0AQMNQUVGhzMxMlZSUKCIiQmFhDXuCQUZBqZ5bvE2S9BDLhgFwE5yxA3BcRUVFmjlzpvr376+QkBC1bt1aXbp0UUREhFq1aqXbb79dq1evNjvmKTMMQxMXbNHh8koltGysG1k2DICboNgBOKb//Oc/at26td58800NHDhQCxYsUHJysrZv364VK1Zo0qRJqqys1MUXX6xLLrlEO3bsMDtyrVUtG5bFsmEA3I7FMAzD7BAAXM+IESP02GOPqVu3bifcr7y8XG+++ab8/Px0yy23OCndqSksLFRoaKgKCgpk+ARo4EvfK6eoXGMvbK8HLu5kdjwAqDMUOwAnVVRUpODghrtu6p+L3eRv9+mDValqG9FIi/5xHitMAHArDMUCOKnzzjtPmZmZZsc4Y2v2HNIHq1IlSZOv7EGpA+B2KHYATqpXr15KSkpSSkpKje3Jycm67LLLTEp16p74coskaUTfWCW1bWpyGgCoexQ7ACf15ptv6uabb9a5556rn376Sdu3b9e1116rxMREeXs3nLNee3NLjiwb1sXsKABQL7iOHYBaefLJJ2W1WnXRRRfJbrfrb3/7m1asWKG+ffuaHe2kdmYXVd9n2TAA7owzdgBOKisrS/fee6+efvppde3aVb6+vrr55psbRKlzOAxN+qxqCHZAp3CWDQPg1ih2AE6qTZs2+uGHH/TRRx9p7dq1mj9/vu644w49//zzZkc7qfd/3acN+wskSY9e1pVlwwC4NYZiAZzU//73P1133XXV319yySX67rvvdPnll2vv3r2aMWOGiemOL6OgVP8+smyYJDVrzLJhANwbZ+wAnNSfS93vEhIS9Msvv2jZsmUmJDo5wzD0+GdVy4b1bBFqdhwAcAqKHYDT1rp1a/3yyy9mxzimxZszteS3qmXDnvy/E6+eAQDugmIH4JhSU1NrtV+TJk0kSenp6fUZ55QUlFbo8c+rJkzc1b+dOkQ13FUzAOBUUOwAHFOfPn105513avXq1cfdp6CgQLNnz1b37t01f/58J6Y7sSlfpSinqFxtwxtpzAXtzY4DAE7D5AkAxzR48GAFBQXpoosukr+/vxITE9W8eXP5+/srLy9Pv/32m7Zs2aKEhAQ999xzLrMCxao/LRv27LCqZcNspSaHAgAnsRiGYZgdAoDr8fPzU1pamoKDgxUREaERI0bo4MGDKi0tVXh4uHr16qVBgwape/fuZketVlZh12Uv/6jducW6rk+splzVU5JUWFio0NBQFRQUKCQkxOSUAFB/OGMH4JiaN2+u5ORkDRo0SKWlpXr22WcVGRlpdqwTemXpDu3OLVZksFUTLmPZMACeh8/YATimBx54QEOGDNF5550ni8Wi999/X6tXr1ZpqWuOa25OL9BrP+yWJD01tDvLhgHwSAzFAjiujRs36osvvtDEiRPVtm1b7d27VxaLRe3bt1dcXJzi4+MVFxenSy+91NSclXaHrpjxs7YcKNRlPaL16g2JNX7OUCwAT0GxA3BSHTp00IoVK9SoUSNt3LhRycnJ1bfNmzerqKjI1Hwzl+/SvxenKDTAV0vGna/IYP8aP6fYAfAUFDsAZ8QwDFPXX92dc1iXvvyjyisdev7qnrqmd+xR+1DsAHgKPmMH4IyYWeocDkPjP9mk8kqHzusQrqsTW5iWBQBcAcUOQIP1wepUrdpzSAG+3nr2yh6mlkwAcAUUOwANUkZBqSYvSpEk/XNQJ8WGBZqcCADMR7ED0OAYhqGJCzbrcHml4mMba9TZrc2OBAAugWIHoMH5cmOGvt2aLV9vi567uqe8vRiCBQCJYgeggckrtumJz7dIksZc0F4do4JNTgQAroNiB6BBeerL33Sw2KZOUcH6+4D2ZscBAJdCsQPQYCzflq1P1qfLYpGmXNVDfj68hQHAn/GuCKBBOFxeqUc/3SxJGn12G/Vq2cTkRADgeih2ABqE5xenKD2/VC2aBOjBQR3NjgMALoliB8Dlrdl7SO+s3CdJmjKspwL9fExOBACuiWIHwKWVVdj18PyNMgzpmsQWOrdDuNmRAMBlUewAuLQZ3+3UrpxihQdZ9djgrmbHAQCXRrED4LK2ZhRq5vJdkqSnruim0EBfkxMBgGuj2AFwSZV2hx6ev1GVDkODukXp0h7NzI4EAC6PYgfAJf3v5z3auL9Awf4+euqK7mbHAYAGgWIHwOXszS3Wf5ZslyQ9NriLIkP8TU4EAA0DxQ6ASzEMQxM+2aSyCofObtdU1/aONTsSADQYFDsALmXe6jSt2H1Q/r5emjKspywWi9mRAKDBoNgBcBlZhWV6ZtFWSdIDF3VSy6aBJicCgIaFYgfAJRiGoYkLNquorFJxLUI1+pzWZkcCgAaHYgfAJXy1OVPf/JYlHy+LplzVUz7evD0BwKninROA082YMUOtW7eWv7+/kpKStOynlXr8s82SpL8PaKcuzUKOedzcuXNlsVg0dOhQJ6YFgIaDYgfAqebNm6dx48Zp0qRJWrduneLi4nTji58q97BN7SODNObC9sc8bu/evXrwwQd13nnnOTkxADQcFsMwDLNDAPAcSUlJ6tOnj6ZPny5J+n5blka9uUaSofl3n63EVmFHHWO323X++efrlltu0Y8//qj8/HwtWLCg1o9ZWFio0NBQFRQUKCTk2GcDAcAdcMYOgNPYbDatXbtWAwcOlCQVl1fq0QVbJEnhh7Ycs9RJ0r/+9S9FRkbq1ltvrdXjlJeXq7CwsMYNADwBxQ6A0+Tm5sputysqKkqS9MI327Q/r1SBRpksm7845jE//fST3njjDc2ePbvWjzN58mSFhoZW32JjucgxAM9AsQNginWpeXrrl72SpN7GdnnZK47ap6ioSDfddJNmz56t8PDwWv/ZEyZMUEFBQfUtLS2trmIDgEvzMTsAAM8RHh4ub29vpWdk6bUVNhmGNCwhRgcXfqjo6Oij9t+1a5f27t2rIUOGVG9zOBySJB8fH23btk3t2rU76jir1Sqr1Vp/TwQAXBTFDoDT+Pn5KTExUa/9lKodfr5q2shPj17WWXHjluqee+45av/OnTtr06ZNNbY99thjKioq0ssvv8wQKwD8BcUOgFMNu+NBvbrNKoukO3s31iMP3Kvi4mKNHj1akjRy5EjFxMRo8uTJ8vf3V/fu3Wsc37hxY0k6ajsAgM/YAXCi4vJKLTwYLouXt4w9v2rs0HOVnJysxYsXV0+oSE1NVUZGhslJAaBh4jp2AJxm/PyNmrs6Tc1D/fXVfecrNMDXKY/LdewAeArO2AFwiiW/ZWnu6jRZLNKL18Y7rdQBgCeh2AGod7mHyzV+/kZJ0m3ntlG/dk1NTgQA7oliB6BeGYah8fM36mCxTZ2jg/XgoE5mRwIAt0WxA1Cv5q1O07dbs+Xn7aWXhsfL6uNtdiQAcFsUOwD1Zm9usf715W+SpAcHdVSXZkxcAID6RLEDUC8q7Q6N+zBZJTa7ktqE6dZz25odCQDcHsUOQL2YuXyX1qXmK9jqoxevjZO3l8XsSADg9ih2AOrcxv35ennpDknSk1d0U4smgSYnAgDPQLEDUKdKbXbdPy9ZlQ5Dl/WI1pW9YsyOBAAeg2IHoE5N+WqrduUUKzLYqmeG9pDFwhAsADgLxQ5Anfl+e47eXrFPkvT8NXFq0sjP5EQA4FkodgDqRF6xTf/8aIMkaVS/VurfMcLkRADgeSh2AM6YYRh6dMEmZReVq21EI42/tIvZkQDAI1HsAJyxBcnpWrQpUz5eFk0dHq8AP1aXAAAzUOwAnJH9eSV6fMEWSdK9f+ugni0amxsIADwYxQ7AaXM4DD3w4QYVlVeqV8vGuntAO7MjAYBHo9gBOG2v/7Rbv+45pEA/b710bbx8vHlLAQAz8S4M4LRszSjUC19vlyRNvLyrWoc3MjkRAIBiB+CUlVVUrS5hszv0t86Ruq5PrNmRAACi2AE4Df9Zsl0pmUVq2shPU67qyeoSAOAiKHYATsmKXQc1+8fdkqQpV/VURLDV5EQAgN9R7ADUWmFZhR78aIMMQ7quT6wu6hpldiQAwJ9Q7ADU2hOfbVF6fqlahgXqscu7mh0HAPAXFDsAtbJwY4Y+WZ8uL4v00vA4BVl9zI4EAPgLih2Ak8osKNMjn26SJP19QHsltgozOREA4FgodgBOqNLu0LgPk1VQWqHuMSH6x986mB0JAHAcFDsAJ/TsohT9suugAv28NXV4vPx8eNsAAFfFOzSA4/p47X797+c9kqQXr4lT+8hgkxMBAE6EYgfgmJLT8qs/V/ePC9vr0h7NTE4EADgZih2Ao2QXlunOd9fIVunQwC5Rum9gR7MjAQBqgWIHoIbySrvufG+tsgrL1T4ySC8Nj5OXF0uGAUBDQLEDUM0wDD2+YIvWp+YrxN9Hs0f2VrC/r9mxAAC1RLEDUO2dFfs0b02avCzStOsT1Ca8kdmRAACngGIHQJK0YtdB/evL3yRJ4y/trP4dI0xOBAA4VRQ7AEo7VKK/v79WdoehofHNdft5bc2OBAA4DRQ7wMOV2Cp1x7trlVdStbLElKt6ymJhsgQANEQUO8CDGYahf360UVszChUe5Kf/3tRb/r7eZscCAJwmih3gwV5dvksLN2XI19uimTcmqnnjALMjAQDOAMUO8FDLUrL0wjfbJElP/F839WkdZnIiAMCZotgBHmhn9mHd+0GyDEO6IamlbkhqZXYkAEAdoNgBHqagtEJ3vLNGReWV6tO6iSYN6WZ2JABAHaHYAR7E7jB039z12p1brOah/nr1hkT5+fA2AADugnd0wIO88M02fbctR1YfL712U29FBFvNjgQAqEMUO8BDfLHhgGYu3yVJeu7qnurRItTkRACAukaxAzzA5vQC/fPjDZKkO89vqyviY0xOBACoDxQ7wM0dPFyuO99dq7IKh/p3jNBDl3Q2OxIAoJ5Q7AA3VmF36O731yk9v1Rtwhvplet6yduL5cIAwF1R7AA39tSXv2nVnkMKsvpo9shEhQb6mh0JAFCPKHaAm5q7KlXvrNgni0WaOjxe7SODzY4EAKhnFDvADa3dd0gTP9ssSRo3sKMGdo0yOREAwBkodoCbySgo1Z3vrlOF3dBlPaJ1z4XtzY4EAHASih3gRsoq7Lrr3bXKPVyuztHBev7qOFksTJYAAE9BsQPchGEYeuSTTdqwv0CNA301e2RvNbL6mB0LAOBEFDvATbzx0x59sj5d3l4WvXp9gmLDAs2OBABwMood4AZ+2pGrZxdtlSQ9NriLzm4fbnIiAIAZKHZAA7fvYLHGzFknhyFdndhCN5/d2uxIAACTUOyABuxweaVuf2eNCkorFB/bWE8P7c5kCQDwYBQ7oIFyOAw98GGytmcdVmSwVa/dlCh/X2+zYwEATESxAxqoact26ustWfLz9tKsmxIVFeJvdiQAgMkodkAD9On6/Xrp2+2SpKev7K6Elk1MTgQAcAUUO6CB+XB1msZ9uEGSNPqc1rq2d6zJiQAAroKrlwINyPu/7tOjn1atAXvjWS01cXBXkxMBAFwJxQ5oIN76eY+e+OI3SVVn6h6/vCszYAEANVDsgAbgvz/s0rOLUiRJd/Zvq/GXdKbUAQCOQrEDXNyM73bq+a+3SZL+cWF73X9RR0odAOCYKHaAizIMQ1O/3aGXl+6QJI27qKP+8bcOJqcCALgyZsUCLsgwDD339bbqUjf+0s5uVepmzJih1q1by9/fX0lJSVq1atVx9509e7bOO+88NWnSRE2aNNHAgQNPuD8AeDKKHeBiDMPQMwu3aubyXZKkiZd31V3925mcqu7MmzdP48aN06RJk7Ru3TrFxcVp0KBBys7OPub+y5cv14gRI/Tdd99pxYoVio2N1cUXX6z09HQnJwcA12cxDMMwOwSAKg6HoSe/2KK3V+yTJD11RTfd1K+1uaHqWFJSkvr06aPp06dLkhwOh2JjYzV27FiNHz/+pMfb7XY1adJE06dP18iRI2v1mIWFhQoNDVVBQYFCQkLOKD8AuDLO2AEuwuEw9OiCTXp7xT5ZLNKUYT3crtTZbDatXbtWAwcOrN7m5eWlgQMHasWKFbX6M0pKSlRRUaGwsLDj7lNeXq7CwsIaNwDwBBQ7wAXYHYYemr9RH6xKk5dFeuHqOF3Xt6XZsepcbm6u7Ha7oqKiamyPiopSZmZmrf6Mhx9+WM2bN69RDv9q8uTJCg0Nrb7FxrI6BwDPQLEDTFZpd2jch8n6eO1+eXtZ9NLweF2V2MLsWC5pypQpmjt3rj799FP5+/sfd78JEyaooKCg+paWlubElABgHi53Apiowu7QfXOTtXBThny8LJo2opcu7dHM7Fj1Jjw8XN7e3srKyqqxPSsrS9HR0Sc89oUXXtCUKVP07bffqmfPnifc12q1ymq1nnFeAGhoOGMHmKS80q6/v79OCzdlyM/bSzNvTHTrUidJfn5+SkxM1NKlS6u3ORwOLV26VP369Tvucc8995yeeuopLV68WL1793ZGVABokDhjB5igrMKuu99bq++25cjPx0uv3ZSoCzpFmh3LKcaNG6dRo0apd+/e6tu3r6ZOnari4mKNHj1akjRy5EjFxMRo8uTJkqR///vfevzxxzVnzhy1bt26+rN4QUFBCgoKMu15AIArotgBTlZqs+uOd9foxx258vf10usj++jcDuFmx3Ka4cOHKycnR48//rgyMzMVHx+vxYsXV0+oSE1NlZfXH4MJM2fOlM1m09VXX13jz5k0aZKeeOIJZ0YHAJfHdewAJyour9Stb6/Wyt2HFOjnrf/d3EdntW1qdiy3x3XsAHgKztgBTlJUVqHRb67Wmn15CrL66K3RfdS79fGvxQYAwKmi2AFOUFBaoVH/W6XktHwF+/vonVv6qlfLJmbHAgC4GYodUM/yS2y66Y1V2pReoMaBvnrv1iR1jwk1OxYAwA1R7IB6dPBwuW58Y5W2ZhQqrJGf3rs1SV2b8xkvAED9oNgB9SS7qEw3vv6rtmcdVniQVXNuT1LHqGCzYwEA3BjFDqgHmQVluv71ldqdU6yoEKvm3H6W2kVwzTUAQP2i2AF1LD2/VNfPXql9B0vUPNRfc24/S63DG5kdCwDgASh2QB1al5qne95fpwMFZYoNC9Cc285SbFig2bEAAB6CYgfUAYfD0Owfd+v5r7ep0mGobXgjvXdbkpo3DjA7GgDAg1DsgDN0qNimBz/aoGUp2ZKky3s20+RhPRTs72tyMgCAp6HYAWdg1Z5D+scH65VZWCY/Hy89MaSbRvSNlcViMTsaAMADUeyA0+BwGJr5/S79Z8l22R2G2kY00ozrE9SlGdeoAwCYh2IHnKLcw+W6f16yftyRK0m6sleMnh7aXY2s/DoBAMzF30TAKfhlV67unZusnKJy+ft66V9XdNc1iS0YegUAuASKHVALdoehact26JWlO+QwpA6RQZpxQwIrSQAAXArFDjiJ7MIy3Ts3WSt2H5QkXdu7hZ78v+4K8PM2ORkAADVR7IAT+HFHju6fl6zcwzYF+nnr6aHdNSyhhdmxAAA4JoodcAyVdoemfrtDM5bvlGFInaODNf36BLWPZL1XAIDrotgBf5FRUKp7P0jWqr2HJEnXJ7XU45d3lb8vQ68AANdGsQP+5LuUbI37MFl5JRUKsvro2WE99H9xzc2OBQBArVDsAEkVdode+HqbXvthtySpe0yIpo9IUOvwRiYnAwCg9ih28Hj780o09oP1Wp+aL0ka1a+VHhncRVYfhl4BAA0LxQ4e7ZstmfrnxxtVUFqhYH8fPXdVT13ao5nZsQAAOC0UO3gkW6VDU75K0f9+3iNJimsRqunXJyg2LNDkZAAAnD6KHTzO2n15euLzLdqUXiBJuvXcNnr4ks7y8/EyORkAAGeGYgePse9gsZ5bvE0LN2VIkkIDfPXCNXG6qGuUyckAAKgbFDu4vbxim6Yt26l3V+5Vhd2QxSJdmxircRd3VFSIv9nxAACoMxQ7uK3ySrve+WWfpi3bocKySknS+R0jNOHSzurSLMTkdAAA1D2KHdyOYRj6YmOGnlucov15pZKqlgR75LIuOr9jhMnpAACoPxQ7uJVVew7pmUVbtSEtX5IUFWLVAxd30lUJLeTtZTE3HAAA9YxiB7ewO+ew/r04RV9vyZIkBfp5667+7XTbeW0U6Mf/5gAAz8DfeGjQDh4u1ytLd+j9X1NV6TDkZZGu69tS9w3soMhgJkYAADwLxQ4NUlmFXW/+vFevfrdTReVVEyMu7BypCZd2VoeoYJPTAQBgDoodGhSHw9DnGw7o+a+3KT2/amJEt+YhevSyLjq7fbjJ6QAAMBfFDg3GL7ty9eyirdqcXihJahbqr38O6qSh8THyYmIEAAAUO7i+ndlFmvJVir7dmi1JCrL66O8XtNMt57SRv6+3yekAAHAdFDu4rJyick39drvmrk6T3WHI28uiG5Ja6h9/66DwIKvZ8QAAcDkUO7ic/BKb3l2xT7O+36Vim12SdFHXKI2/tLPaRQSZnA4AANdFsYPL2JCWr3dX7tMXGw6ovNIhSYprEapHLuuipLZNTU4HAIDro9jBVKU2u77YeEDvrdynjfsLqrd3bRaiO/u31ZCezZkYAQBALVHsYIo9ucV6f+U+fbR2vwpKKyRJft5eGtyzmW48q5USWjaWxUKhAwDgVFDs4DSVdoeWpWTr3ZX79OOO3OrtLZoE6IakVrq2dws1ZVIEAACnjWKHepddVKYPV6dpzq+pOlBQJkmyWKQLOkXqprNa6fyOEfJmuBUAgDNGsUO9MAxDq/Yc0rsr92nx5kxVOgxJUpNAXw3v01I3JLVUbFigySkBAHAvFDvUqaKyCi1Yn653V+7T9qzD1dsTWjbWTf1a6dLuzbioMAAA9YRihzNWVmHX99tztHhzpr7Zkll97bkAX28N7dVcNyS1UveYUJNTAgDg/ih2OC2Hyyu1LCVbizdn6LuUHJVW2Kt/1jaikW46q5WGJbRQaICviSkBAPAsFDvUWn6JTUt+y9LXWzL1w45c2Y5cRFiSYhoHaFC3aF3aI1q9WzXhUiUAAJiAYocTyi4q0zdbqsrcil0HqydBSFKb8Ea6pHu0Lu0erR4xoZQ5AABMRrHDUdLzS/X15kwt3pyp1fsOyfijy6lzdPCRMtdMHaOCKHMAALgQih1UVmHXutQ8/br7kJZvy9aGPy3tJVWt13pJ92a6pHu02oQ3MiklAAA4GYqdByq12bV2X55+3XNQv+4+pOS0fNnsf3xezmKR+rQK0yXdozWoe7RiGgeYmBYAANQWxc4DFJdXau2+PK3cfVC/7jmkjfvzVWE3auwTFWLVWW2bql/bpvpblyhFBLO0FwAADQ3Fzg0dLq/U6r2H9OvuQ/p1z0Ft2l9QY9KDJDUL9ddZbZsqqU2YzmrbVK2aBvJ5OQAAGjiKnRsoLKvQmiNFbuXug9p8oFD2vxS5mMYBVUWubZjOatNUsWEBFDkAANwMxa6BKSqr0LbMIm3NLFJKRqE27i/QlgMF+kuPU8uwQCW1CVPSkbNyrMsKAID7o9i5KLvD0N6DxUrJKFJKZqG2Hvm6P6/0mPu3bhqopDZNdVa7MCW1aarmTHgAAMDjUOxcQF6xTVszC6tLXEpmkbZnFamswnHM/ZuF+qtzdLA6NwtRl2Yh6ts6TNGh/k5ODQAAXA3Fzokq7A7tzimucQYuJaNImYVlx9zf39dLnaJD1CU6uLrIdY4OVuNAPycnBwAADQHFro6VVdi1P69UaXkl2n+opPr+ntwS7cwuOuoyI79rGRb4x1m4I19bhgXK24sJDgAAoHYodqeowu5QRn6Z0vJKlPan4pZ2qERpeaXKKSo/4fFBVp8jBS5YnaOrhlI7RQcryMpLAQAAzgxt4i8cDkNZRWVKO1R6VHHbn1eqjILSo2ag/lWQ1UctmgSoRZNAxYYFKLZJoFqGBapTdLBaNOEyIwAAoH54RLEzDEPFNrtyisqrb7mH/7ifc/iPbbmHy487XPo7q4/XUcUtNixQLZpU3W8c6Et5AwAATtdgi52t0qGC0goVlNpUUFqhg4dtNQraX0vb8WaYHouPl0XNGwdUF7XYsIAaxS08yCovPvsGAABcjKnFrtL+ezmruuWXVqjw9+9Lqr7/888LSyuUX1J1v7TCfsqP18jPWxHB1upbeJBVEUHWo7ZFBlvl4+1VD88YAACg/pxSsauwO1Ris6vEVqni8qqvx/7eruLyyj++VthVUl6pYptdRWWV1eXtcHnlGYW3WKRgq49CA30VFuiniGB/RQT7HVXWIoL8FR7sp0C/BnuCEgAA4KRq3XQ6PvaVbJW1H848FUFWH4UG+Na4NQ6s+hryl+//fAv29+VyIEADNGPGDD3//PPKzMxUXFycpk2bpr59+x53/48++kgTJ07U3r171aFDB/373//WZZdd5sTEANAw1LrY/bnU+XhZFOjnrUZWnxpfA/2OfO/no0Crd/W2Rn7eCjyyT5DVR40D/arLWYi/D8OegAeZN2+exo0bp1mzZikpKUlTp07VoEGDtG3bNkVGRh61/y+//KIRI0Zo8uTJuvzyyzVnzhwNHTpU69atU/fu3U14BgDguiyGYZzk4h1V0vNLqwqan4/8fChiAE5PUlKS+vTpo+nTp0uSHA6HYmNjNXbsWI0fP/6o/YcPH67i4mJ9+eWX1dvOOussxcfHa9asWbV6zMLCQoWGhqqgoEAhISF180QAwAXV6oydYRgK9qqQKitUVikdewEsADgxm82mNWvW6N5771VhYWH19vPPP18//PCD/v73vx91zM8//6x77rmnxv4DBgzQl19+WWPbn5WXl6u8/I+LhRcVFUnScfcHgIYgODj4pJdTq9UZu9//tQsAAABz1GbUoVbFzjCM6n/xeorCwkLFxsYqLS2NoRs3xuvsXBkZGercubOWLFlSY7LExIkT9fPPP2vZsmVHHdO0aVPNmjVL11xzTfW22bNna8qUKdq1a9cxH+evZ+wyMjLUt29f/fbbb4qJianDZwRXwu+zZ/Dk17k2Z+xqNRRrsVg87j/e70JCQjz2uXsSXmfn8Pf3l7e3tw4fPlzjv3d+fr5iYmKO+Ro0a9ZMRUVFNX5WWFio5s2bn/JrFhwczOvsAfh99gy8zsfGLAgATuPn56fExEQtXbq0epvD4dDSpUvVr1+/Yx7Tr1+/GvtL0pIlS467PwB4Mq7YC8Cpxo0bp1GjRql3797q27evpk6dquLiYo0ePVqSNHLkSMXExGjy5MmSpHvvvVf9+/fXiy++qMGDB2vu3Llas2aN/vvf/5r5NADAJVHsjsNqtWrSpEmyWq1mR0E94nV2vuHDhysnJ0ePP/64MjMzFR8fr8WLFysqKkqSlJqaKi+vPwYTzj77bM2ZM0ePPfaYHnnkEXXo0EELFiw4pWvY/f768jq7N36fPQOv84nV+jp2ANBQcR07AJ6Cz9gBAAC4CYodAACAm6DYAQAAuAmKHQAAgJug2J2C8vJyxcfHy2KxKDk52ew4qEN79+7VrbfeqjZt2iggIEDt2rXTpEmTZLPZzI6GOvD7pVEiIiKUlJSkVatWmZwIdWny5Mnq06ePgoODFRkZqaFDh2rbtm1mx0I9mjJliiwWi+677z6zo7gcit0peOihh9S8eXOzY6AepKSkyOFw6LXXXtOWLVv00ksvadasWXrkkUfMjoYzNG/evOrX8ccff1RcXJwGDRqk7Oxsk5Ohrnz//fcaM2aMVq5cqSVLlqiiokIXX3yxiouLzY6GerB69Wq99tpr6tmzp9lRXBKXO6mlr776SuPGjdP8+fPVrVs3rV+/XvHx8WbHQj16/vnnNXPmTO3evdvsKDgDSUlJiouL0+zZs1VQUKCgoCDFxsZq7NixGj9+vNnxUA9ycnIUGRmp77//Xueff77ZcVCHDh8+rISEBL366qt6+umnFR8fr6lTp5ody6Vwxq4WsrKydPvtt+vdd99VYGCg2XHgJAUFBQoLCzM7Bs6AzWbT2rVrNWDAgOptXl5eGjhwoFasWGFeMNSrgoICSeL31w2NGTNGgwcP1sCBA82O4rJYeeIkDMPQzTffrLvuuku9e/fW3r17zY4EJ9i5c6emTZumF154wewoOAO5ubmy2+2KjIyssT0qKkopKSkmpUJ9cjgcuu+++3TOOeec0uokcH1z587VunXrtHr1arOjuDSPPWM3fvx4WSyWE95SUlI0bdo0FRUVacKECWZHxmmo7ev8Z+np6brkkkt0zTXX6PbbbzcpOYDTMWbMGG3evFlz5841OwrqUFpamu699169//778vf3NzuOS/PYz9jl5OTo4MGDJ9ynbdu2uvbaa/XFF1/IYrFUb7fb7fL29tYNN9ygt99+u76j4gzU9nX28/OTJB04cEADBgzQWWedpbfeeqvGmqVoeGw2mwIDA/XOO+/ohhtuqF5SbNSoUcrPz9dnn31mdkTUoXvuuUefffaZfvjhB7Vp08bsOKhDCxYs0JVXXilvb+/qbXa7XRaLRV5eXiovL6/xM0/mscWutlJTU1VYWFj9/YEDBzRo0CB9/PHHSkpKUosWLUxMh7qUnp6uCy64QImJiXrvvfd4k3ATSUlJio+P13//+9/qyRMtW7bUPffcw+QJN2EYhsaOHatPP/1Uy5cvV4cOHcyOhDpWVFSkffv21dg2evRode7cWQ8//DDD7n/CZ+xOomXLljW+DwoKkiS1a9eOUudG0tPTNWDAALVq1UovvPCCcnJyqn8WHR1tYjKcqXHjxmnkyJGSpG3btun1119XcXGxRo8ebXIy1JUxY8Zozpw5+uyzzxQcHKzMzExJUmhoqAICAkxOh7oQHBx8VHlr1KiRmjZtSqn7C4odIGnJkiXauXOndu7ceVRh56R2wzZ8+HBlZ2frueee0znnnKNevXpp8eLFioqKMjsa6sjMmTMlqcbsZ0l68803dfPNNzs/EGAihmIBAADcBJ8MBwAAcBMUOwAAADdBsQMAAHATFDsAAAA3QbEDAABwExQ7AAAAN0GxAwAAcBMUOwAAADdBsQMAAHATFDsAAAA3QbEDAABwExQ7AG7rgw8+UEBAgDIyMqq3jR49Wj179lRBQYGJyQCgflgMwzDMDgEA9cEwDMXHx+v888/XtGnTNGnSJP3vf//TypUrFRMTY3Y8AKhzPmYHAID6YrFY9Mwzz+jqq69WdHS0pk2bph9//JFSB8BtccYOgNtLSEjQli1b9M0336h///5mxwGAesNn7AC4tcWLFyslJUV2u11RUVFmxwGAesUZOwBua926dRowYIBee+01vfXWWwoJCdFHH31kdiwAqDd8xg6AW9q7d68GDx6sRx55RCNGjFDbtm3Vr18/rVu3TgkJCWbHA4B6wRk7AG7n0KFDOvvsszVgwADNmjWrevvgwYNlt9u1ePFiE9MBQP2h2AEAALgJJk8AAAC4CYodAACAm6DYAQAAuAmKHQAAgJug2AEAALgJih0AAICboNgBAAC4CYodAACAm6DYAQAAuAmKHQAAgJug2AEAALiJ/wdI5ZS7Lat8lgAAAABJRU5ErkJggg==","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["<sympy.plotting.plot.Plot at 0x2b0bbe6e0>"]},"execution_count":168,"metadata":{},"output_type":"execute_result"}],"source":["import sympy\n","sympy.plot(\"1/(1+exp(-x))\", xlim=(-5,5))"]},{"cell_type":"code","execution_count":169,"metadata":{},"outputs":[],"source":["def create_predictions(features: torch.Tensor, coefficients: torch.Tensor) -> torch.Tensor:\n","    summed_weighted_values = (coefficients * features).sum(dim=1)\n","    return torch.sigmoid(summed_weighted_values)\n"]},{"cell_type":"markdown","metadata":{},"source":["Constricting the range of our predictions within the range of what they can realistically be makes them much easier to optimize. When this is applied to every prediction each epoch should minimize our loss more effectivelyby by eliminating values that are outside the range of what our predictions can realistically be.\n","\n","This in turn allows us to substantially increase the learning rate as our loss won't be as high or fluctuate as wildly."]},{"cell_type":"code","execution_count":170,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["0.451; 0.324; 0.299; 0.209; 0.200; 0.198; 0.197; 0.197; 0.196; 0.196; 0.196; 0.195; 0.195; 0.195; 0.195; 0.195; 0.195; 0.195; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; "]},{"data":{"text/plain":["tensor(0.8258)"]},"execution_count":170,"metadata":{},"output_type":"execute_result"}],"source":["coefficients = train_model(learning_rate=100)\n","calculate_accuracy(coefficients, features=validation_features)"]},{"cell_type":"markdown","metadata":{},"source":["83% a sharp improvement!"]},{"cell_type":"markdown","metadata":{},"source":["## Prepare Submission CSV"]},{"cell_type":"markdown","metadata":{},"source":["### Using a Test set\n","Before submitting to Kaggle we'll want to test the effectiveness of our data against our test set. \n","#### Why not use the Validation set?\n","This may seem similar to how we used our validation set but there's an important difference. A validation set is used to give us an unbiased evaluation of our model's performance. Unlike a training set which is biased as we're training our model on it. As we develop our model the validation set will indirectly become biased as we iterate on our model to improve the validation sets accuracy. The test set is only ever used once we have finished developing our model so it gives us an accurate assessment of how our model behaves on completely unseen data.\n","\n","- Training set - Model bias\n","- Validation Set - Developer bias\n","- Test set - No bias"]},{"cell_type":"markdown","metadata":{},"source":["### Clean Test Data"]},{"cell_type":"code","execution_count":171,"metadata":{},"outputs":[{"data":{"text/plain":["PassengerId      0\n","Pclass           0\n","Name             0\n","Sex              0\n","Age             86\n","SibSp            0\n","Parch            0\n","Ticket           0\n","Fare             1\n","Cabin          327\n","Embarked         0\n","dtype: int64"]},"execution_count":171,"metadata":{},"output_type":"execute_result"}],"source":["test_df = pd.read_csv(data_path + 'test.csv')\n","test_df.isna().sum()"]},{"cell_type":"markdown","metadata":{},"source":["We can use the same steps we took for cleaning the training data on our test data. However it's always worth checking if there are any additional na values. Here there's an na value for Fare that needs resolving."]},{"cell_type":"code","execution_count":172,"metadata":{},"outputs":[{"data":{"text/plain":["PassengerId    0\n","Name           0\n","Age            0\n","SibSp          0\n","Parch          0\n","Ticket         0\n","Fare           0\n","Cabin          0\n","Sex_female     0\n","Sex_male       0\n","Embarked_C     0\n","Embarked_Q     0\n","Embarked_S     0\n","Pclass_1       0\n","Pclass_2       0\n","Pclass_3       0\n","LogFare        0\n","dtype: int64"]},"execution_count":172,"metadata":{},"output_type":"execute_result"}],"source":["test_df['Fare'] = test_df.Fare.fillna(0)\n","test_df = substitue_na_with_modes(test_df)\n","test_df = convert_categories_to_binary_values(test_df)\n","test_df[\"LogFare\"] = np.log(test_df['Fare'] + 1)\n","test_df.isna().sum()"]},{"cell_type":"code","execution_count":173,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Age</th>\n","      <th>SibSp</th>\n","      <th>Parch</th>\n","      <th>LogFare</th>\n","      <th>Sex_male</th>\n","      <th>Sex_female</th>\n","      <th>Pclass_1</th>\n","      <th>Pclass_2</th>\n","      <th>Pclass_3</th>\n","      <th>Embarked_C</th>\n","      <th>Embarked_Q</th>\n","      <th>Embarked_S</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>34.5</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2.178064</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>47.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>2.079442</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>62.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2.369075</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>27.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2.268252</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>22.0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>2.586824</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    Age  SibSp  Parch   LogFare  Sex_male  Sex_female  Pclass_1  Pclass_2  Pclass_3  Embarked_C  Embarked_Q  Embarked_S\n","0  34.5      0      0  2.178064         1           0         0         0         1           0           1           0\n","1  47.0      1      0  2.079442         0           1         0         0         1           0           0           1\n","2  62.0      0      0  2.369075         1           0         0         1         0           0           1           0\n","3  27.0      0      0  2.268252         1           0         0         0         1           0           0           1\n","4  22.0      1      1  2.586824         0           1         0         0         1           0           0           1"]},"execution_count":173,"metadata":{},"output_type":"execute_result"}],"source":["test_df[feature_names][:5]"]},{"cell_type":"code","execution_count":174,"metadata":{},"outputs":[{"data":{"text/plain":["tensor([[0.4313, 0.0000, 0.0000, 0.3490, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000],\n","        [0.5875, 0.1250, 0.0000, 0.3332, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000],\n","        [0.7750, 0.0000, 0.0000, 0.3796, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000]])"]},"execution_count":174,"metadata":{},"output_type":"execute_result"}],"source":["test_features = tensor(test_df[feature_names].values, dtype=torch.float)\n","test_features = test_features / max_values\n","test_features[:3]"]},{"cell_type":"code","execution_count":175,"metadata":{},"outputs":[{"data":{"text/plain":["tensor([[0.2750, 0.1250, 0.0000, 0.3381, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000],\n","        [0.4750, 0.1250, 0.0000, 0.6859, 0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000],\n","        [0.3250, 0.0000, 0.0000, 0.3507, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000],\n","        [0.4375, 0.1250, 0.0000, 0.6395, 0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000],\n","        [0.4375, 0.0000, 0.0000, 0.3530, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000],\n","        [0.3000, 0.0000, 0.0000, 0.3600, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000],\n","        [0.6750, 0.0000, 0.0000, 0.6358, 1.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000],\n","        ...,\n","        [0.3125, 0.0000, 0.0000, 0.3342, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000],\n","        [0.4875, 0.0000, 0.8333, 0.5456, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000],\n","        [0.3375, 0.0000, 0.0000, 0.4229, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000],\n","        [0.2375, 0.0000, 0.0000, 0.5502, 0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000],\n","        [0.3000, 0.1250, 0.3333, 0.5122, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000],\n","        [0.3250, 0.0000, 0.0000, 0.5502, 1.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000],\n","        [0.4000, 0.0000, 0.0000, 0.3476, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000]])"]},"execution_count":175,"metadata":{},"output_type":"execute_result"}],"source":["feature_tensor"]},{"cell_type":"code","execution_count":176,"metadata":{},"outputs":[{"data":{"text/plain":["tensor([ -0.7410,  -1.1686,  -0.5568,   0.5259, -10.4339,   8.4665,   3.0045,   1.5414,  -6.7864,   1.8826,   2.2974,  -4.6379],\n","       requires_grad=True)"]},"execution_count":176,"metadata":{},"output_type":"execute_result"}],"source":["coefficients"]},{"cell_type":"markdown","metadata":{},"source":["### Create test predictions"]},{"cell_type":"code","execution_count":177,"metadata":{},"outputs":[{"data":{"text/plain":["tensor([0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n","        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,\n","        1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,\n","        1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,\n","        1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,\n","        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,\n","        0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n","        0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0], dtype=torch.int32)"]},"execution_count":177,"metadata":{},"output_type":"execute_result"}],"source":["with torch.no_grad():\n","    test_predictions = create_predictions(test_features, coefficients=coefficients)\n","test_predictions = (test_predictions > 0.5).int()\n","test_predictions"]},{"cell_type":"markdown","metadata":{},"source":["### Create Kaggle submission\n","We can view the sample submission kaggle has given us to see how to format this."]},{"cell_type":"code","execution_count":178,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>PassengerId</th>\n","      <th>Survived</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>892</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>893</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>894</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   PassengerId  Survived\n","0          892         0\n","1          893         1\n","2          894         0"]},"execution_count":178,"metadata":{},"output_type":"execute_result"}],"source":["sample_df = pd.read_csv(data_path + \"gender_submission.csv\")\n","sample_df[:3]"]},{"cell_type":"code","execution_count":179,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["PassengerId,Survived\n","892,0\n","893,0\n","894,0\n","895,0\n","896,0\n","897,0\n","898,1\n","899,0\n","900,1\n"]}],"source":["submission_df = pd.DataFrame({ \"PassengerId\": test_df[\"PassengerId\"], \"Survived\": test_predictions })\n","submission = submission_df.to_csv(\"submission.csv\",index=False)\n","!head submission.csv"]},{"cell_type":"markdown","metadata":{},"source":["## Neural Nets\n","The calculation above was a linear regression as we only use one set of parameters in the form of our features which we immedietely derived the output from. This effectively gave us a single layer with one neuron.\n","\n","Here we'll employ two additional concepts to improve the resolution of our calculations accuracy. We will create a single hidden layer with multiple neurons, apply a RELU (Rectified Linear Unit) function and add them together to give us a loss. A RELU function is non-linear and simply replaces every negative number with a 0."]},{"cell_type":"markdown","metadata":{},"source":["### Create Matrix of Coefficients"]},{"cell_type":"markdown","metadata":{},"source":["Prior to now we were using a single neuron consisting of a single layer. We're now going to add additional neurons by generating more than one set of coefficients\n","\n","Our first layer will take our inputs/features and create 20 outputs from 20 different sets of coefficients."]},{"cell_type":"code","execution_count":180,"metadata":{},"outputs":[{"data":{"text/plain":["torch.Size([12, 20])"]},"execution_count":180,"metadata":{},"output_type":"execute_result"}],"source":["hidden_layer_neuron_count = 20\n","layer1 = torch.rand(feature_count, hidden_layer_neuron_count) - 0.5\n","layer1.shape"]},{"cell_type":"markdown","metadata":{},"source":["When we've more than one neuron we need to adjust the values of each neuron proportionally so we end up with a similar magnitude."]},{"cell_type":"code","execution_count":181,"metadata":{},"outputs":[{"data":{"text/plain":["tensor([[ 0.0239, -0.0101, -0.0043,  0.0095, -0.0041, -0.0049, -0.0207,  0.0067, -0.0151,  0.0009,  0.0244, -0.0077, -0.0079,  0.0151,\n","         -0.0092, -0.0021,  0.0233, -0.0102, -0.0179, -0.0140],\n","        [-0.0069, -0.0119, -0.0130,  0.0101,  0.0042, -0.0080, -0.0194, -0.0079, -0.0106, -0.0081, -0.0226,  0.0054, -0.0184, -0.0195,\n","         -0.0204,  0.0104, -0.0151, -0.0103,  0.0196,  0.0133],\n","        [ 0.0143, -0.0237, -0.0179, -0.0094,  0.0207,  0.0026, -0.0187,  0.0002, -0.0194, -0.0055, -0.0069,  0.0216,  0.0077, -0.0044,\n","          0.0042, -0.0072,  0.0098,  0.0099,  0.0067, -0.0097]])"]},"execution_count":181,"metadata":{},"output_type":"execute_result"}],"source":["layer1 = layer1 / hidden_layer_neuron_count\n","layer1[:3]"]},{"cell_type":"markdown","metadata":{},"source":["### Creating an output layer\n","The 2nd layer is our output layer which will produce one output predicting the passengers survival. As we're using neurons from a hidden layer as our input and not our features which included dummy values we now need to include a constant (also known as a bias). This 2nd layer consists of one neuron"]},{"cell_type":"code","execution_count":182,"metadata":{},"outputs":[{"data":{"text/plain":["torch.Size([20, 1])"]},"execution_count":182,"metadata":{},"output_type":"execute_result"}],"source":["layer2 = torch.rand(hidden_layer_neuron_count, 1) - 0.3\n","constant = torch.rand(1)[0]\n","layer2.shape"]},{"cell_type":"code","execution_count":183,"metadata":{},"outputs":[],"source":["def generate_coefficients(features: torch.Tensor, hidden_layer_neuron_count: int=20):\n","    feature_count = features.shape[1]\n","    layer1 = (torch.rand(feature_count, hidden_layer_neuron_count) - 0.5) / feature_count\n","    layer2 = torch.rand(hidden_layer_neuron_count, 1) - 0.3\n","    constant = torch.rand(1)[0]\n","    return layer1.requires_grad_(), layer2.requires_grad_(), constant.requires_grad_()"]},{"cell_type":"markdown","metadata":{},"source":["### Convert targets to column vectors\n","Lastly we need to add an additional dimension to our targets so matrix multiplications will work correctly on them.  We need to be very careful to remember to do this as not doing so won't return an error but will give us incorrect results as the Pytorch will attempt to broadcast the mishaped target tensor."]},{"cell_type":"code","execution_count":184,"metadata":{},"outputs":[],"source":["training_targets = training_targets[:,None]\n","validation_targets = validation_targets[:,None]"]},{"cell_type":"markdown","metadata":{},"source":["### Matrix Multiplication\n","Now that we have more than one neuron (more than one row of parameters/coefficients) we need to switch from using pytorch's broadcasting to matrix multiplication so that every row of parameters is multiplied with our features. It's also worth noting using PyTorch's matrix multiplication `@` is very efficient as it's been highly optimized both algorithmically and for operating on multi-core GPUs. You'll see us using this later when we create the neural net.\n","\n","```python\n","torch.sigmoid(coefficients @ features)\n","```"]},{"cell_type":"markdown","metadata":{},"source":["### Updating multiple coefficients\n","We also need to loop through our coefficients now we have more than one layer. We simpy add a loop to our existing function."]},{"cell_type":"code","execution_count":185,"metadata":{},"outputs":[],"source":["def update_coefficients(coefficients, learning_rate):\n","    for layer in coefficients:\n","        layer.sub_(layer.grad * learning_rate)\n","        layer.grad.zero_()"]},{"cell_type":"markdown","metadata":{},"source":["### Creating the Neural net\n","Our plan for this neural net is to have two layers.\n","\n","12 Features -> |20 neuron hidden layer| --(Relu)-> |1 neuron output layer| --(Sigmoid)-> 1 Survival prediction"]},{"cell_type":"code","execution_count":186,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Dean, Master. Bertram Vere\n","torch.Size([12]) --> torch.Size([12, 20]) --> torch.Size([20, 1]) --> 1.0225906372070312\n"]}],"source":["print(df.Name[788])\n","print(str(feature_tensor[0].shape) + \" --> \" + str(layer1.shape) + \" --> \" + str(layer2.shape) + \" --> \" + str(predictions[0].item()))"]},{"cell_type":"markdown","metadata":{},"source":["If you're curious [Bertram](https://titanic.fandom.com/wiki/Bertram_Vere_Dean) did indeed survive and lived until 1992."]},{"cell_type":"markdown","metadata":{},"source":["\n","The RELU is needed as adding together two linear functions just gives us another linear function which doesn't give us any more resolution for our calculation. Combining each linear layer with a non-linear RELU allows us to keep each linear functions utility increasing our algortihms accuracy."]},{"cell_type":"code","execution_count":187,"metadata":{},"outputs":[],"source":["import torch.nn.functional as F\n","\n","def create_predictions(features: torch.Tensor, coefficients: tuple[Tensor, Tensor, Tensor]) -> torch.Tensor:\n","    layer1_parameters, layer2_parameters, constant_parameter = coefficients\n","    layer1_output = F.relu(features @ layer1_parameters)\n","    layer2_output = torch.sigmoid(layer1_output @ layer2_parameters + constant_parameter)\n","    return layer2_output"]},{"cell_type":"code","execution_count":188,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["0.537; 0.437; 0.298; 0.245; 0.215; 0.215; 0.215; 0.215; 0.215; 0.215; 0.214; 0.214; 0.213; 0.206; 0.204; 0.208; 0.195; 0.194; 0.194; 0.194; 0.193; 0.193; 0.193; 0.193; 0.193; 0.193; 0.193; 0.193; 0.193; 0.193; "]}],"source":["coefficients = train_model(learning_rate=20)"]},{"cell_type":"code","execution_count":189,"metadata":{},"outputs":[{"data":{"text/plain":["tensor(0.8258)"]},"execution_count":189,"metadata":{},"output_type":"execute_result"}],"source":["calculate_accuracy(coefficients, features=validation_features)"]},{"cell_type":"markdown","metadata":{},"source":["### Linear Regression vs Neural Net\n","Our 82.58% accuracy is by (I compared my results against fast.ai's where the same thing happeneed to them) coincidence the same result as we got from the linear model. Usually a neural net will have improved performance against a linear model. However for relatively small simple datasets such as the titanic dataset linear regression will usually do just fine and less complexity isn't neeeded."]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"databundleVersionId":26502,"sourceId":3136,"sourceType":"competition"}],"dockerImageVersionId":30558,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
